{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini_Projet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn3gzrlKI1KO",
        "outputId": "42b029b1-d019-419f-9822-23f4c7947b69"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkFwW3Yv2UyT"
      },
      "source": [
        "Commençons par charger et lire un exemple de fichier audio "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCvXBPweJRlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa91a845-95ce-4114-e76c-4e18f766d2db"
      },
      "source": [
        "!pip install librosa\n",
        "import librosa\n",
        "from librosa import display\n",
        "data, sampling_rate = librosa.load('/content/drive/My Drive/Ravdess/Audio_Speech_Actors_01-24/Actor_01/03-01-01-01-01-01-01.wav')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.19.5)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (2.4.7)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T8mpIEX2X5O"
      },
      "source": [
        "Examinons ensuite un graphique d'onde de ce fichier audio en utilisant librosa.display.waveplot "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGsJ-unLKDG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "315884fc-7b3e-486a-8a80-6858d02354f4"
      },
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7fdf374f87d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTjfWBDS2IBy"
      },
      "source": [
        "What is audio sampling rate?\n",
        "The sampling rate refers to the number of samples of audio recorded every second. It is measured in samples per second or Hertz (abbreviated as Hz or kHz, with one kHz being 1000 Hz). An audio sample is just a number representing the measured acoustic wave value at a specific point in time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-QPS-MFMzl0"
      },
      "source": [
        "**Charger tout les fichier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plUTQnM96SQZ"
      },
      "source": [
        " **os.path.join**(path, *paths)\n",
        "\n",
        "  Join one or more path components intelligently. The return value is the concatenation of path and any members of *paths with exactly one directory separator following each non-empty part except the last, meaning that the result will only end in a separator if the last part is empty. If a component is an absolute path, all previous components are thrown away and joining continues from the absolute path component.\n",
        "\n",
        "\n",
        " **numpy.mean**(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)\n",
        "\n",
        "  Compute the arithmetic mean along the specified axis.\n",
        "\n",
        "  Returns the average of the array elements. The average is taken over the flattened array by default, otherwise over the specified axis. float64 intermediate and return values are used for integer inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqtn1W3BMtYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d05e334-ed06-46af-9924-3a2f833ee951"
      },
      "source": [
        "import time\n",
        "path = '/content/drive/My Drive/Ravdess/'\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Data loaded. Loading time: 1636.514396905899 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUej7T4WBtZ0"
      },
      "source": [
        "The zip() function returns a zip object, which is an iterator of tuples where the first item in each passed iterator is paired together, and then the second item in each passed iterator are paired together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8le93bq5T8Et"
      },
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoFhqCNwUALy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "905ccb5d-62a0-4e94-f7eb-c9e4a9341f06"
      },
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2497, 40), (2497,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xQOa8sbUKMD"
      },
      "source": [
        "# Saving joblib files to not load them again with the loop above\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_name = 'X.joblib'\n",
        "y_name = 'y.joblib'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "\n",
        "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
        "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW5YHSq_Uk_o"
      },
      "source": [
        "# Loading saved models\n",
        "\n",
        "X = joblib.load('/content/drive/My Drive/Ravdess_model/X.joblib')\n",
        "y = joblib.load('/content/drive/My Drive/Ravdess_model/y.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2GRgwFOVERW"
      },
      "source": [
        "**Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGcSO1gB9pA-"
      },
      "source": [
        "L’apprentissage par arbre de décision désigne une méthode basée sur l'utilisation d'un arbre de décision comme modèle prédictif. On l'utilise notamment en fouille de données et en apprentissage automatique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0h3_i6pUpP7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVgssCdAELlp"
      },
      "source": [
        "**fit**(X, y, sample_weight=None, check_input=True,X_idx_sorted='deprecated')\n",
        "\n",
        "Build a decision tree classifier from the training set (X, y).\n",
        "\n",
        "**X**{array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "  The training input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csc_matrix.\n",
        "  \n",
        "**y** array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
        "\n",
        "  The target values (class labels) as integers or strings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir0Hkul4VWjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dedea0b-d75f-4d4c-8e59-c466a5b94b0f"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7ndyDcaVt5P"
      },
      "source": [
        "predictions = dtree.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-ztm5MwV7y-"
      },
      "source": [
        "emotions = { \"neutral\": \"0\", \"calm\": \"1\", \"happy\": \"2\", \"sad\": \"3\", \"angry\": \"4\", \"fearful\": \"5\", \"disgust\": \"6\", \"surprised\": \"7\" }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKdCdp0ZV9x1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333cd407-7d70-495d-8f73-1bfc8dd0eb0e"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.43      0.44        70\n",
            "           1       0.53      0.58      0.56       115\n",
            "           2       0.45      0.48      0.46       121\n",
            "           3       0.40      0.44      0.42       118\n",
            "           4       0.54      0.54      0.54       117\n",
            "           5       0.43      0.43      0.43       134\n",
            "           6       0.24      0.24      0.24        67\n",
            "           7       0.38      0.27      0.31        83\n",
            "\n",
            "    accuracy                           0.44       825\n",
            "   macro avg       0.43      0.42      0.43       825\n",
            "weighted avg       0.44      0.44      0.44       825\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOXFBQJ-WQh-"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cff2kBLFNFF"
      },
      "source": [
        "L'algorithme des forêts d'arbres décisionnels effectue un apprentissage sur de multiples arbres de décision entraînés sur des sous-ensembles de données légèrement différents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx_O6O1eWR8o"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy3Ksq20WbGz"
      },
      "source": [
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39KbO7ujWdeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c263d97-4c54-4b57-cfd2-cc1846ac37f4"
      },
      "source": [
        "rforest.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=10, max_features='log2',\n",
              "                       max_leaf_nodes=100, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=3, min_samples_split=20,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=22000,\n",
              "                       n_jobs=None, oob_score=False, random_state=5, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcKWryK5W8RI"
      },
      "source": [
        "predictions = rforest.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X3H5tyfW-dr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ead6ec-11fd-4e7f-ad02-588eede39060"
      },
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.44      0.61        70\n",
            "           1       0.56      0.90      0.69       115\n",
            "           2       0.58      0.58      0.58       121\n",
            "           3       0.56      0.58      0.57       118\n",
            "           4       0.73      0.85      0.78       117\n",
            "           5       0.60      0.57      0.58       134\n",
            "           6       0.35      0.28      0.31        67\n",
            "           7       0.74      0.42      0.54        83\n",
            "\n",
            "    accuracy                           0.61       825\n",
            "   macro avg       0.64      0.58      0.58       825\n",
            "weighted avg       0.63      0.61      0.60       825\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhuVB7OhFf97"
      },
      "source": [
        "**Neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWptHAFKXJUG"
      },
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gCZY4ChXQts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5863b3-a37e-4c3e-9d8a-20219b0daee1"
      },
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1672, 40, 1), (825, 40, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qbZUrUyNKdM"
      },
      "source": [
        "Le modèle **séquentiel** est une pile linéaire de couches.\n",
        "\n",
        "Vous pouvez créer un modèle séquentiel en passant au constructeur une liste d’instances de couches .\n",
        "Vous pouvez également les ajouter à un modèle existant avec la méthode .add() :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF9Au77SXXQY"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(8))\n",
        "model.add(Activation('softmax'))\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcpQR3IxYU-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944c0db9-0b66-4056-be35-0966e02f55c5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VwG7njEYeek"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L82g5rTnYiVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2be442c7-b0ae-4a46-f3ac-8780c514ac6c"
      },
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "105/105 [==============================] - 2s 10ms/step - loss: 7.3364 - accuracy: 0.1304 - val_loss: 2.7527 - val_accuracy: 0.1648\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 6.1661 - accuracy: 0.1334 - val_loss: 2.7340 - val_accuracy: 0.1503\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 5.4871 - accuracy: 0.1507 - val_loss: 2.6945 - val_accuracy: 0.2085\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 4.8244 - accuracy: 0.1717 - val_loss: 2.3599 - val_accuracy: 0.2242\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 4.2137 - accuracy: 0.1687 - val_loss: 2.0548 - val_accuracy: 0.2485\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 3.7572 - accuracy: 0.1699 - val_loss: 2.1557 - val_accuracy: 0.2242\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 3.3199 - accuracy: 0.1836 - val_loss: 2.1356 - val_accuracy: 0.1927\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 3.0056 - accuracy: 0.1980 - val_loss: 2.1858 - val_accuracy: 0.2267\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 2.6977 - accuracy: 0.1998 - val_loss: 2.1150 - val_accuracy: 0.1745\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 2.5228 - accuracy: 0.2099 - val_loss: 2.1121 - val_accuracy: 0.2109\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 2.3146 - accuracy: 0.2362 - val_loss: 1.9246 - val_accuracy: 0.2667\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 2.2088 - accuracy: 0.2309 - val_loss: 1.8151 - val_accuracy: 0.2533\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 2.1070 - accuracy: 0.2434 - val_loss: 1.8404 - val_accuracy: 0.2461\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 2.0503 - accuracy: 0.2536 - val_loss: 1.7901 - val_accuracy: 0.3224\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.9787 - accuracy: 0.2614 - val_loss: 1.7957 - val_accuracy: 0.3091\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.9424 - accuracy: 0.2542 - val_loss: 1.7960 - val_accuracy: 0.3055\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.8777 - accuracy: 0.2961 - val_loss: 1.7621 - val_accuracy: 0.4012\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.8373 - accuracy: 0.2883 - val_loss: 1.7566 - val_accuracy: 0.3442\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.7983 - accuracy: 0.3206 - val_loss: 1.7595 - val_accuracy: 0.3273\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.8350 - accuracy: 0.3032 - val_loss: 1.7420 - val_accuracy: 0.3236\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.7985 - accuracy: 0.3224 - val_loss: 1.7227 - val_accuracy: 0.3479\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.7961 - accuracy: 0.3278 - val_loss: 1.7187 - val_accuracy: 0.3745\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.8026 - accuracy: 0.3116 - val_loss: 1.7077 - val_accuracy: 0.3709\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.7508 - accuracy: 0.3242 - val_loss: 1.6957 - val_accuracy: 0.3673\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.7491 - accuracy: 0.3289 - val_loss: 1.6952 - val_accuracy: 0.3479\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.7254 - accuracy: 0.3457 - val_loss: 1.6606 - val_accuracy: 0.4012\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.7004 - accuracy: 0.3439 - val_loss: 1.6617 - val_accuracy: 0.4182\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.6772 - accuracy: 0.3439 - val_loss: 1.6664 - val_accuracy: 0.3758\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.6947 - accuracy: 0.3439 - val_loss: 1.6544 - val_accuracy: 0.3988\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.6698 - accuracy: 0.3792 - val_loss: 1.6391 - val_accuracy: 0.4024\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.6720 - accuracy: 0.3624 - val_loss: 1.6264 - val_accuracy: 0.4303\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.6443 - accuracy: 0.3756 - val_loss: 1.6161 - val_accuracy: 0.4121\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.6386 - accuracy: 0.3732 - val_loss: 1.6075 - val_accuracy: 0.4218\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.6246 - accuracy: 0.3780 - val_loss: 1.5978 - val_accuracy: 0.4109\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.6052 - accuracy: 0.3977 - val_loss: 1.5739 - val_accuracy: 0.4315\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.6114 - accuracy: 0.3864 - val_loss: 1.5858 - val_accuracy: 0.3806\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.6001 - accuracy: 0.3870 - val_loss: 1.5686 - val_accuracy: 0.4206\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.5795 - accuracy: 0.4007 - val_loss: 1.5742 - val_accuracy: 0.4206\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.5695 - accuracy: 0.4085 - val_loss: 1.5621 - val_accuracy: 0.4582\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.5580 - accuracy: 0.3941 - val_loss: 1.5436 - val_accuracy: 0.4255\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.5487 - accuracy: 0.4127 - val_loss: 1.5357 - val_accuracy: 0.4473\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.5364 - accuracy: 0.4181 - val_loss: 1.5356 - val_accuracy: 0.4436\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.5420 - accuracy: 0.4222 - val_loss: 1.5674 - val_accuracy: 0.4303\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.5009 - accuracy: 0.4330 - val_loss: 1.5244 - val_accuracy: 0.4339\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.4985 - accuracy: 0.4438 - val_loss: 1.5294 - val_accuracy: 0.4400\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4854 - accuracy: 0.4426 - val_loss: 1.5281 - val_accuracy: 0.4376\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.5079 - accuracy: 0.4240 - val_loss: 1.5092 - val_accuracy: 0.4545\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4890 - accuracy: 0.4342 - val_loss: 1.4909 - val_accuracy: 0.4545\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4950 - accuracy: 0.4234 - val_loss: 1.4848 - val_accuracy: 0.4485\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.5011 - accuracy: 0.4234 - val_loss: 1.4816 - val_accuracy: 0.4739\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4597 - accuracy: 0.4581 - val_loss: 1.4723 - val_accuracy: 0.4752\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4824 - accuracy: 0.4252 - val_loss: 1.4815 - val_accuracy: 0.4727\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4596 - accuracy: 0.4599 - val_loss: 1.4688 - val_accuracy: 0.4739\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4432 - accuracy: 0.4629 - val_loss: 1.4808 - val_accuracy: 0.4642\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4394 - accuracy: 0.4641 - val_loss: 1.4730 - val_accuracy: 0.4691\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.4240 - accuracy: 0.4653 - val_loss: 1.4537 - val_accuracy: 0.4764\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4257 - accuracy: 0.4635 - val_loss: 1.4423 - val_accuracy: 0.5018\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4143 - accuracy: 0.4767 - val_loss: 1.4285 - val_accuracy: 0.5030\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4175 - accuracy: 0.4641 - val_loss: 1.4330 - val_accuracy: 0.4885\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4262 - accuracy: 0.4659 - val_loss: 1.4353 - val_accuracy: 0.4897\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3872 - accuracy: 0.4964 - val_loss: 1.4312 - val_accuracy: 0.4764\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.4183 - accuracy: 0.4713 - val_loss: 1.4142 - val_accuracy: 0.5103\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3799 - accuracy: 0.4791 - val_loss: 1.4154 - val_accuracy: 0.4982\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3924 - accuracy: 0.4755 - val_loss: 1.4180 - val_accuracy: 0.4933\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3885 - accuracy: 0.4982 - val_loss: 1.4292 - val_accuracy: 0.4824\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3674 - accuracy: 0.4779 - val_loss: 1.4027 - val_accuracy: 0.4848\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.3588 - accuracy: 0.4940 - val_loss: 1.4180 - val_accuracy: 0.4945\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3424 - accuracy: 0.4958 - val_loss: 1.4314 - val_accuracy: 0.4824\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.3599 - accuracy: 0.4934 - val_loss: 1.4141 - val_accuracy: 0.4836\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3427 - accuracy: 0.5132 - val_loss: 1.3955 - val_accuracy: 0.4945\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3293 - accuracy: 0.5132 - val_loss: 1.3832 - val_accuracy: 0.4994\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.3351 - accuracy: 0.5090 - val_loss: 1.3791 - val_accuracy: 0.5067\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3108 - accuracy: 0.5102 - val_loss: 1.3682 - val_accuracy: 0.5079\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3198 - accuracy: 0.5096 - val_loss: 1.3765 - val_accuracy: 0.5030\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3090 - accuracy: 0.5066 - val_loss: 1.3645 - val_accuracy: 0.5224\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3169 - accuracy: 0.5156 - val_loss: 1.3590 - val_accuracy: 0.5115\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.3138 - accuracy: 0.5078 - val_loss: 1.3496 - val_accuracy: 0.5261\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2960 - accuracy: 0.5072 - val_loss: 1.3512 - val_accuracy: 0.5188\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 1.2902 - accuracy: 0.5299 - val_loss: 1.3768 - val_accuracy: 0.4994\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2952 - accuracy: 0.5275 - val_loss: 1.3487 - val_accuracy: 0.5224\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2899 - accuracy: 0.5263 - val_loss: 1.3717 - val_accuracy: 0.5055\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.2782 - accuracy: 0.5365 - val_loss: 1.3470 - val_accuracy: 0.5200\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2706 - accuracy: 0.5299 - val_loss: 1.3568 - val_accuracy: 0.5358\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.2610 - accuracy: 0.5269 - val_loss: 1.3317 - val_accuracy: 0.5285\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2686 - accuracy: 0.5299 - val_loss: 1.3397 - val_accuracy: 0.5200\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2605 - accuracy: 0.5263 - val_loss: 1.3231 - val_accuracy: 0.5345\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2539 - accuracy: 0.5419 - val_loss: 1.3180 - val_accuracy: 0.5442\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2417 - accuracy: 0.5377 - val_loss: 1.3624 - val_accuracy: 0.4909\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2297 - accuracy: 0.5520 - val_loss: 1.3212 - val_accuracy: 0.5285\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2373 - accuracy: 0.5478 - val_loss: 1.3182 - val_accuracy: 0.5406\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2324 - accuracy: 0.5478 - val_loss: 1.3337 - val_accuracy: 0.5333\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2413 - accuracy: 0.5431 - val_loss: 1.3385 - val_accuracy: 0.5115\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2255 - accuracy: 0.5502 - val_loss: 1.3296 - val_accuracy: 0.5115\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2310 - accuracy: 0.5425 - val_loss: 1.3065 - val_accuracy: 0.5321\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2174 - accuracy: 0.5550 - val_loss: 1.3094 - val_accuracy: 0.5261\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2182 - accuracy: 0.5455 - val_loss: 1.3028 - val_accuracy: 0.5394\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2157 - accuracy: 0.5467 - val_loss: 1.3079 - val_accuracy: 0.5345\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2053 - accuracy: 0.5640 - val_loss: 1.3017 - val_accuracy: 0.5479\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.2005 - accuracy: 0.5496 - val_loss: 1.2874 - val_accuracy: 0.5358\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1975 - accuracy: 0.5670 - val_loss: 1.2786 - val_accuracy: 0.5382\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1890 - accuracy: 0.5622 - val_loss: 1.3066 - val_accuracy: 0.5236\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1934 - accuracy: 0.5544 - val_loss: 1.2982 - val_accuracy: 0.5297\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1915 - accuracy: 0.5550 - val_loss: 1.2865 - val_accuracy: 0.5564\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1731 - accuracy: 0.5694 - val_loss: 1.2928 - val_accuracy: 0.5430\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1892 - accuracy: 0.5592 - val_loss: 1.2996 - val_accuracy: 0.5333\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1678 - accuracy: 0.5658 - val_loss: 1.2594 - val_accuracy: 0.5515\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1746 - accuracy: 0.5724 - val_loss: 1.2861 - val_accuracy: 0.5455\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1760 - accuracy: 0.5885 - val_loss: 1.2753 - val_accuracy: 0.5467\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1662 - accuracy: 0.5807 - val_loss: 1.2681 - val_accuracy: 0.5576\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1816 - accuracy: 0.5718 - val_loss: 1.2823 - val_accuracy: 0.5309\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1580 - accuracy: 0.5778 - val_loss: 1.2665 - val_accuracy: 0.5370\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1636 - accuracy: 0.5742 - val_loss: 1.2596 - val_accuracy: 0.5600\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1401 - accuracy: 0.5801 - val_loss: 1.2653 - val_accuracy: 0.5515\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1382 - accuracy: 0.5778 - val_loss: 1.2574 - val_accuracy: 0.5503\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1322 - accuracy: 0.5873 - val_loss: 1.2624 - val_accuracy: 0.5467\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1464 - accuracy: 0.5903 - val_loss: 1.2646 - val_accuracy: 0.5382\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1313 - accuracy: 0.5885 - val_loss: 1.2414 - val_accuracy: 0.5503\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 1.1257 - accuracy: 0.5885 - val_loss: 1.2363 - val_accuracy: 0.5673\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1312 - accuracy: 0.5909 - val_loss: 1.2568 - val_accuracy: 0.5576\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1182 - accuracy: 0.5861 - val_loss: 1.2467 - val_accuracy: 0.5564\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1310 - accuracy: 0.5855 - val_loss: 1.2394 - val_accuracy: 0.5539\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1267 - accuracy: 0.5843 - val_loss: 1.2277 - val_accuracy: 0.5576\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1148 - accuracy: 0.5801 - val_loss: 1.2375 - val_accuracy: 0.5564\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1122 - accuracy: 0.5933 - val_loss: 1.2623 - val_accuracy: 0.5479\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1042 - accuracy: 0.5861 - val_loss: 1.2386 - val_accuracy: 0.5588\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1001 - accuracy: 0.5921 - val_loss: 1.2356 - val_accuracy: 0.5636\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1124 - accuracy: 0.5945 - val_loss: 1.2496 - val_accuracy: 0.5515\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1000 - accuracy: 0.6035 - val_loss: 1.2370 - val_accuracy: 0.5588\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0964 - accuracy: 0.5951 - val_loss: 1.2307 - val_accuracy: 0.5733\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0937 - accuracy: 0.6065 - val_loss: 1.2389 - val_accuracy: 0.5491\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0885 - accuracy: 0.6011 - val_loss: 1.2226 - val_accuracy: 0.5648\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.1039 - accuracy: 0.5933 - val_loss: 1.2021 - val_accuracy: 0.5806\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0989 - accuracy: 0.5945 - val_loss: 1.2202 - val_accuracy: 0.5685\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0820 - accuracy: 0.6112 - val_loss: 1.2190 - val_accuracy: 0.5770\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0506 - accuracy: 0.6172 - val_loss: 1.2520 - val_accuracy: 0.5406\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 1.0829 - accuracy: 0.5969 - val_loss: 1.2116 - val_accuracy: 0.5673\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0606 - accuracy: 0.6160 - val_loss: 1.2094 - val_accuracy: 0.5709\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0782 - accuracy: 0.6094 - val_loss: 1.1982 - val_accuracy: 0.5818\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0516 - accuracy: 0.6142 - val_loss: 1.2142 - val_accuracy: 0.5745\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0503 - accuracy: 0.6232 - val_loss: 1.2125 - val_accuracy: 0.5685\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0692 - accuracy: 0.6124 - val_loss: 1.1930 - val_accuracy: 0.5770\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0499 - accuracy: 0.6112 - val_loss: 1.1989 - val_accuracy: 0.5758\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0526 - accuracy: 0.6214 - val_loss: 1.1894 - val_accuracy: 0.5806\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0493 - accuracy: 0.6262 - val_loss: 1.1806 - val_accuracy: 0.5806\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0399 - accuracy: 0.6286 - val_loss: 1.1888 - val_accuracy: 0.5745\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 1.0354 - accuracy: 0.6250 - val_loss: 1.1983 - val_accuracy: 0.5564\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 1.0333 - accuracy: 0.6244 - val_loss: 1.1968 - val_accuracy: 0.5745\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0300 - accuracy: 0.6292 - val_loss: 1.1842 - val_accuracy: 0.5745\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0305 - accuracy: 0.6214 - val_loss: 1.1750 - val_accuracy: 0.5745\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0337 - accuracy: 0.6184 - val_loss: 1.1822 - val_accuracy: 0.5770\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0152 - accuracy: 0.6376 - val_loss: 1.1997 - val_accuracy: 0.5721\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0190 - accuracy: 0.6286 - val_loss: 1.1765 - val_accuracy: 0.5806\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0176 - accuracy: 0.6417 - val_loss: 1.1816 - val_accuracy: 0.5733\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0145 - accuracy: 0.6262 - val_loss: 1.1904 - val_accuracy: 0.5758\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 1.0139 - accuracy: 0.6334 - val_loss: 1.1817 - val_accuracy: 0.5758\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 1.0302 - accuracy: 0.6244 - val_loss: 1.1685 - val_accuracy: 0.5818\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0123 - accuracy: 0.6417 - val_loss: 1.1767 - val_accuracy: 0.5830\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0148 - accuracy: 0.6286 - val_loss: 1.1642 - val_accuracy: 0.5952\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9859 - accuracy: 0.6406 - val_loss: 1.1764 - val_accuracy: 0.5806\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0083 - accuracy: 0.6453 - val_loss: 1.1683 - val_accuracy: 0.5939\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9897 - accuracy: 0.6435 - val_loss: 1.1819 - val_accuracy: 0.5927\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0147 - accuracy: 0.6262 - val_loss: 1.1476 - val_accuracy: 0.6036\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 1s 7ms/step - loss: 0.9953 - accuracy: 0.6334 - val_loss: 1.1517 - val_accuracy: 0.5891\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0072 - accuracy: 0.6376 - val_loss: 1.1659 - val_accuracy: 0.5855\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 1.0021 - accuracy: 0.6382 - val_loss: 1.1886 - val_accuracy: 0.5733\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9691 - accuracy: 0.6543 - val_loss: 1.1565 - val_accuracy: 0.5976\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9937 - accuracy: 0.6286 - val_loss: 1.1692 - val_accuracy: 0.5903\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9951 - accuracy: 0.6304 - val_loss: 1.1448 - val_accuracy: 0.5842\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9717 - accuracy: 0.6543 - val_loss: 1.1623 - val_accuracy: 0.5927\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9821 - accuracy: 0.6459 - val_loss: 1.1559 - val_accuracy: 0.5818\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9865 - accuracy: 0.6358 - val_loss: 1.1581 - val_accuracy: 0.5867\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9658 - accuracy: 0.6519 - val_loss: 1.1590 - val_accuracy: 0.5842\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9614 - accuracy: 0.6507 - val_loss: 1.1508 - val_accuracy: 0.5915\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9672 - accuracy: 0.6585 - val_loss: 1.1451 - val_accuracy: 0.5976\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9570 - accuracy: 0.6531 - val_loss: 1.1538 - val_accuracy: 0.6012\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9505 - accuracy: 0.6495 - val_loss: 1.1428 - val_accuracy: 0.6012\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9515 - accuracy: 0.6513 - val_loss: 1.1342 - val_accuracy: 0.5964\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9552 - accuracy: 0.6549 - val_loss: 1.1493 - val_accuracy: 0.5770\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9569 - accuracy: 0.6507 - val_loss: 1.1373 - val_accuracy: 0.6000\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9505 - accuracy: 0.6669 - val_loss: 1.1355 - val_accuracy: 0.6121\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9485 - accuracy: 0.6621 - val_loss: 1.1571 - val_accuracy: 0.5891\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9458 - accuracy: 0.6651 - val_loss: 1.1201 - val_accuracy: 0.6061\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9422 - accuracy: 0.6573 - val_loss: 1.1169 - val_accuracy: 0.6145\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9392 - accuracy: 0.6609 - val_loss: 1.1397 - val_accuracy: 0.5867\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9383 - accuracy: 0.6609 - val_loss: 1.1805 - val_accuracy: 0.5697\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9477 - accuracy: 0.6579 - val_loss: 1.1403 - val_accuracy: 0.5952\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9446 - accuracy: 0.6633 - val_loss: 1.1488 - val_accuracy: 0.5879\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9335 - accuracy: 0.6675 - val_loss: 1.1462 - val_accuracy: 0.5952\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9410 - accuracy: 0.6591 - val_loss: 1.1328 - val_accuracy: 0.6073\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9342 - accuracy: 0.6681 - val_loss: 1.1234 - val_accuracy: 0.6061\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9357 - accuracy: 0.6597 - val_loss: 1.1156 - val_accuracy: 0.6085\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9271 - accuracy: 0.6645 - val_loss: 1.1331 - val_accuracy: 0.5818\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9087 - accuracy: 0.6794 - val_loss: 1.1169 - val_accuracy: 0.6073\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9140 - accuracy: 0.6675 - val_loss: 1.1088 - val_accuracy: 0.6145\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9141 - accuracy: 0.6603 - val_loss: 1.1123 - val_accuracy: 0.6036\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9147 - accuracy: 0.6591 - val_loss: 1.1635 - val_accuracy: 0.5939\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9237 - accuracy: 0.6663 - val_loss: 1.1181 - val_accuracy: 0.6061\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9055 - accuracy: 0.6711 - val_loss: 1.1191 - val_accuracy: 0.6012\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9093 - accuracy: 0.6717 - val_loss: 1.1116 - val_accuracy: 0.5952\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8967 - accuracy: 0.6752 - val_loss: 1.1169 - val_accuracy: 0.6000\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.9037 - accuracy: 0.6705 - val_loss: 1.1141 - val_accuracy: 0.6061\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8941 - accuracy: 0.6818 - val_loss: 1.1082 - val_accuracy: 0.6061\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8966 - accuracy: 0.6830 - val_loss: 1.1245 - val_accuracy: 0.5964\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9066 - accuracy: 0.6806 - val_loss: 1.1192 - val_accuracy: 0.5939\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8958 - accuracy: 0.6788 - val_loss: 1.1119 - val_accuracy: 0.6073\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.9077 - accuracy: 0.6788 - val_loss: 1.1334 - val_accuracy: 0.6085\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8830 - accuracy: 0.6860 - val_loss: 1.0980 - val_accuracy: 0.6145\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8963 - accuracy: 0.6788 - val_loss: 1.1176 - val_accuracy: 0.5952\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8704 - accuracy: 0.6776 - val_loss: 1.1162 - val_accuracy: 0.6061\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8920 - accuracy: 0.6746 - val_loss: 1.1127 - val_accuracy: 0.6121\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8870 - accuracy: 0.6902 - val_loss: 1.1209 - val_accuracy: 0.5976\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8789 - accuracy: 0.6866 - val_loss: 1.0912 - val_accuracy: 0.6048\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8794 - accuracy: 0.6824 - val_loss: 1.1275 - val_accuracy: 0.6109\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8579 - accuracy: 0.6920 - val_loss: 1.1495 - val_accuracy: 0.5758\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8719 - accuracy: 0.6836 - val_loss: 1.1068 - val_accuracy: 0.6170\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8812 - accuracy: 0.6818 - val_loss: 1.1254 - val_accuracy: 0.5964\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8741 - accuracy: 0.6788 - val_loss: 1.1168 - val_accuracy: 0.5939\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8645 - accuracy: 0.6860 - val_loss: 1.1207 - val_accuracy: 0.6012\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8691 - accuracy: 0.6890 - val_loss: 1.0983 - val_accuracy: 0.6206\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8582 - accuracy: 0.6854 - val_loss: 1.1013 - val_accuracy: 0.6085\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8552 - accuracy: 0.6884 - val_loss: 1.0986 - val_accuracy: 0.6012\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8690 - accuracy: 0.6902 - val_loss: 1.1194 - val_accuracy: 0.5952\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8621 - accuracy: 0.6992 - val_loss: 1.1154 - val_accuracy: 0.6242\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8566 - accuracy: 0.6896 - val_loss: 1.0921 - val_accuracy: 0.6073\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8469 - accuracy: 0.6926 - val_loss: 1.1311 - val_accuracy: 0.5855\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8522 - accuracy: 0.6992 - val_loss: 1.0858 - val_accuracy: 0.6170\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8459 - accuracy: 0.6932 - val_loss: 1.1364 - val_accuracy: 0.5952\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8530 - accuracy: 0.6860 - val_loss: 1.0924 - val_accuracy: 0.6073\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8207 - accuracy: 0.7081 - val_loss: 1.1125 - val_accuracy: 0.6012\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8337 - accuracy: 0.7063 - val_loss: 1.1140 - val_accuracy: 0.6024\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8419 - accuracy: 0.7022 - val_loss: 1.0860 - val_accuracy: 0.6206\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8407 - accuracy: 0.6932 - val_loss: 1.0935 - val_accuracy: 0.6109\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8434 - accuracy: 0.6914 - val_loss: 1.0857 - val_accuracy: 0.6170\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8250 - accuracy: 0.7022 - val_loss: 1.0899 - val_accuracy: 0.6085\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8466 - accuracy: 0.6902 - val_loss: 1.0903 - val_accuracy: 0.6109\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8212 - accuracy: 0.7117 - val_loss: 1.0708 - val_accuracy: 0.6206\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8153 - accuracy: 0.7141 - val_loss: 1.0937 - val_accuracy: 0.6133\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8271 - accuracy: 0.6950 - val_loss: 1.1037 - val_accuracy: 0.6061\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8247 - accuracy: 0.7016 - val_loss: 1.0974 - val_accuracy: 0.6291\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8302 - accuracy: 0.7063 - val_loss: 1.0878 - val_accuracy: 0.6048\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8065 - accuracy: 0.7171 - val_loss: 1.1219 - val_accuracy: 0.6000\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8272 - accuracy: 0.7069 - val_loss: 1.1106 - val_accuracy: 0.5988\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8079 - accuracy: 0.7141 - val_loss: 1.0969 - val_accuracy: 0.6073\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8028 - accuracy: 0.7075 - val_loss: 1.0772 - val_accuracy: 0.6206\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8053 - accuracy: 0.7039 - val_loss: 1.0663 - val_accuracy: 0.6218\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7849 - accuracy: 0.7183 - val_loss: 1.0701 - val_accuracy: 0.6230\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8194 - accuracy: 0.7004 - val_loss: 1.0747 - val_accuracy: 0.6255\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8010 - accuracy: 0.7213 - val_loss: 1.0830 - val_accuracy: 0.6170\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7942 - accuracy: 0.7087 - val_loss: 1.0769 - val_accuracy: 0.6073\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8131 - accuracy: 0.7087 - val_loss: 1.0670 - val_accuracy: 0.6218\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.8076 - accuracy: 0.7177 - val_loss: 1.0683 - val_accuracy: 0.6182\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.8085 - accuracy: 0.7010 - val_loss: 1.0631 - val_accuracy: 0.6073\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7851 - accuracy: 0.7147 - val_loss: 1.0686 - val_accuracy: 0.6109\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7969 - accuracy: 0.7022 - val_loss: 1.0960 - val_accuracy: 0.6085\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7959 - accuracy: 0.7153 - val_loss: 1.0856 - val_accuracy: 0.6194\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7983 - accuracy: 0.7081 - val_loss: 1.0686 - val_accuracy: 0.6194\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7832 - accuracy: 0.7165 - val_loss: 1.0753 - val_accuracy: 0.6012\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7798 - accuracy: 0.7201 - val_loss: 1.0696 - val_accuracy: 0.6024\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.7822 - accuracy: 0.7231 - val_loss: 1.0942 - val_accuracy: 0.6109\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7887 - accuracy: 0.7231 - val_loss: 1.0663 - val_accuracy: 0.6158\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7802 - accuracy: 0.7117 - val_loss: 1.0588 - val_accuracy: 0.6194\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7737 - accuracy: 0.7177 - val_loss: 1.0625 - val_accuracy: 0.6170\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7779 - accuracy: 0.7267 - val_loss: 1.0858 - val_accuracy: 0.6109\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7654 - accuracy: 0.7327 - val_loss: 1.0614 - val_accuracy: 0.6242\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7804 - accuracy: 0.7237 - val_loss: 1.0628 - val_accuracy: 0.6024\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7602 - accuracy: 0.7309 - val_loss: 1.0869 - val_accuracy: 0.6182\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7590 - accuracy: 0.7315 - val_loss: 1.0520 - val_accuracy: 0.6242\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7547 - accuracy: 0.7309 - val_loss: 1.0764 - val_accuracy: 0.6085\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.7721 - accuracy: 0.7225 - val_loss: 1.0716 - val_accuracy: 0.6133\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7602 - accuracy: 0.7237 - val_loss: 1.0552 - val_accuracy: 0.6194\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7564 - accuracy: 0.7291 - val_loss: 1.0784 - val_accuracy: 0.6194\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7785 - accuracy: 0.7189 - val_loss: 1.0771 - val_accuracy: 0.6024\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7436 - accuracy: 0.7416 - val_loss: 1.0523 - val_accuracy: 0.6182\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7451 - accuracy: 0.7321 - val_loss: 1.0520 - val_accuracy: 0.6158\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7479 - accuracy: 0.7410 - val_loss: 1.0512 - val_accuracy: 0.6303\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7641 - accuracy: 0.7255 - val_loss: 1.0544 - val_accuracy: 0.6206\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7732 - accuracy: 0.7249 - val_loss: 1.0686 - val_accuracy: 0.6048\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.7564 - accuracy: 0.7207 - val_loss: 1.0872 - val_accuracy: 0.6061\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7425 - accuracy: 0.7285 - val_loss: 1.0856 - val_accuracy: 0.5927\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7633 - accuracy: 0.7291 - val_loss: 1.0719 - val_accuracy: 0.6097\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7330 - accuracy: 0.7386 - val_loss: 1.0746 - val_accuracy: 0.6182\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7488 - accuracy: 0.7362 - val_loss: 1.0518 - val_accuracy: 0.6255\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7355 - accuracy: 0.7386 - val_loss: 1.0685 - val_accuracy: 0.6194\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7344 - accuracy: 0.7339 - val_loss: 1.0428 - val_accuracy: 0.6279\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7298 - accuracy: 0.7356 - val_loss: 1.0503 - val_accuracy: 0.6158\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7335 - accuracy: 0.7374 - val_loss: 1.0576 - val_accuracy: 0.6182\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7341 - accuracy: 0.7410 - val_loss: 1.0602 - val_accuracy: 0.6267\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7502 - accuracy: 0.7350 - val_loss: 1.0399 - val_accuracy: 0.6194\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.7263 - accuracy: 0.7398 - val_loss: 1.0363 - val_accuracy: 0.6267\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7260 - accuracy: 0.7416 - val_loss: 1.0633 - val_accuracy: 0.6291\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7373 - accuracy: 0.7458 - val_loss: 1.0721 - val_accuracy: 0.6206\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7314 - accuracy: 0.7368 - val_loss: 1.0528 - val_accuracy: 0.6170\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7038 - accuracy: 0.7440 - val_loss: 1.0775 - val_accuracy: 0.6158\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.7248 - accuracy: 0.7380 - val_loss: 1.0541 - val_accuracy: 0.6133\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.7148 - accuracy: 0.7386 - val_loss: 1.0372 - val_accuracy: 0.6352\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7203 - accuracy: 0.7350 - val_loss: 1.0488 - val_accuracy: 0.6182\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7123 - accuracy: 0.7428 - val_loss: 1.0579 - val_accuracy: 0.6048\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7087 - accuracy: 0.7530 - val_loss: 1.0436 - val_accuracy: 0.6182\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7157 - accuracy: 0.7422 - val_loss: 1.0681 - val_accuracy: 0.6291\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6945 - accuracy: 0.7578 - val_loss: 1.0392 - val_accuracy: 0.6194\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7082 - accuracy: 0.7476 - val_loss: 1.0293 - val_accuracy: 0.6255\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7248 - accuracy: 0.7428 - val_loss: 1.0596 - val_accuracy: 0.6133\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.7186 - accuracy: 0.7458 - val_loss: 1.0338 - val_accuracy: 0.6327\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7217 - accuracy: 0.7380 - val_loss: 1.0386 - val_accuracy: 0.6267\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7140 - accuracy: 0.7321 - val_loss: 1.0443 - val_accuracy: 0.6291\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7193 - accuracy: 0.7356 - val_loss: 1.0257 - val_accuracy: 0.6352\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7050 - accuracy: 0.7422 - val_loss: 1.0494 - val_accuracy: 0.6279\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6988 - accuracy: 0.7542 - val_loss: 1.0557 - val_accuracy: 0.6291\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6996 - accuracy: 0.7536 - val_loss: 1.0472 - val_accuracy: 0.6170\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7113 - accuracy: 0.7434 - val_loss: 1.0354 - val_accuracy: 0.6303\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7081 - accuracy: 0.7380 - val_loss: 1.0506 - val_accuracy: 0.6230\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.7044 - accuracy: 0.7542 - val_loss: 1.0429 - val_accuracy: 0.6279\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.7101 - accuracy: 0.7422 - val_loss: 1.0576 - val_accuracy: 0.6291\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6971 - accuracy: 0.7572 - val_loss: 1.0494 - val_accuracy: 0.6182\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6914 - accuracy: 0.7518 - val_loss: 1.0343 - val_accuracy: 0.6279\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6815 - accuracy: 0.7572 - val_loss: 1.0336 - val_accuracy: 0.6133\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6886 - accuracy: 0.7518 - val_loss: 1.0398 - val_accuracy: 0.6145\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6849 - accuracy: 0.7506 - val_loss: 1.0511 - val_accuracy: 0.6291\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6846 - accuracy: 0.7542 - val_loss: 1.0341 - val_accuracy: 0.6267\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6858 - accuracy: 0.7512 - val_loss: 1.0384 - val_accuracy: 0.6315\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6929 - accuracy: 0.7494 - val_loss: 1.0320 - val_accuracy: 0.6170\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6755 - accuracy: 0.7530 - val_loss: 1.0491 - val_accuracy: 0.6376\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6837 - accuracy: 0.7661 - val_loss: 1.0561 - val_accuracy: 0.6291\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6849 - accuracy: 0.7434 - val_loss: 1.0272 - val_accuracy: 0.6424\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6749 - accuracy: 0.7590 - val_loss: 1.0343 - val_accuracy: 0.6279\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6727 - accuracy: 0.7530 - val_loss: 1.0620 - val_accuracy: 0.6085\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6798 - accuracy: 0.7518 - val_loss: 1.0544 - val_accuracy: 0.6364\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6838 - accuracy: 0.7554 - val_loss: 1.0215 - val_accuracy: 0.6279\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6475 - accuracy: 0.7763 - val_loss: 1.0681 - val_accuracy: 0.6024\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6720 - accuracy: 0.7572 - val_loss: 1.0466 - val_accuracy: 0.6303\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6745 - accuracy: 0.7542 - val_loss: 1.0613 - val_accuracy: 0.6182\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6732 - accuracy: 0.7685 - val_loss: 1.0212 - val_accuracy: 0.6279\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6672 - accuracy: 0.7656 - val_loss: 1.0294 - val_accuracy: 0.6230\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6584 - accuracy: 0.7578 - val_loss: 1.0588 - val_accuracy: 0.6279\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6898 - accuracy: 0.7542 - val_loss: 1.0282 - val_accuracy: 0.6412\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6644 - accuracy: 0.7685 - val_loss: 1.0590 - val_accuracy: 0.6085\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6723 - accuracy: 0.7620 - val_loss: 1.0568 - val_accuracy: 0.6133\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6573 - accuracy: 0.7679 - val_loss: 1.0425 - val_accuracy: 0.6291\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6463 - accuracy: 0.7817 - val_loss: 1.0555 - val_accuracy: 0.6303\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6564 - accuracy: 0.7656 - val_loss: 1.0536 - val_accuracy: 0.6267\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6326 - accuracy: 0.7847 - val_loss: 1.0368 - val_accuracy: 0.6242\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6551 - accuracy: 0.7602 - val_loss: 1.0511 - val_accuracy: 0.6255\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6508 - accuracy: 0.7703 - val_loss: 1.0568 - val_accuracy: 0.6242\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6635 - accuracy: 0.7602 - val_loss: 1.0563 - val_accuracy: 0.6279\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6477 - accuracy: 0.7799 - val_loss: 1.0630 - val_accuracy: 0.6242\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6441 - accuracy: 0.7721 - val_loss: 1.0106 - val_accuracy: 0.6303\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6561 - accuracy: 0.7626 - val_loss: 1.0449 - val_accuracy: 0.6291\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6462 - accuracy: 0.7799 - val_loss: 1.0347 - val_accuracy: 0.6315\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6632 - accuracy: 0.7656 - val_loss: 1.0200 - val_accuracy: 0.6400\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6479 - accuracy: 0.7679 - val_loss: 1.0366 - val_accuracy: 0.6291\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6363 - accuracy: 0.7703 - val_loss: 1.0616 - val_accuracy: 0.6182\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6372 - accuracy: 0.7799 - val_loss: 1.0389 - val_accuracy: 0.6291\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6295 - accuracy: 0.7835 - val_loss: 1.0482 - val_accuracy: 0.6352\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6483 - accuracy: 0.7679 - val_loss: 1.0202 - val_accuracy: 0.6303\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6270 - accuracy: 0.7745 - val_loss: 1.0427 - val_accuracy: 0.6158\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6338 - accuracy: 0.7721 - val_loss: 1.0312 - val_accuracy: 0.6267\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.6443 - accuracy: 0.7769 - val_loss: 1.0332 - val_accuracy: 0.6376\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6431 - accuracy: 0.7835 - val_loss: 1.0297 - val_accuracy: 0.6339\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6259 - accuracy: 0.7763 - val_loss: 1.0330 - val_accuracy: 0.6291\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6329 - accuracy: 0.7787 - val_loss: 1.0258 - val_accuracy: 0.6339\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6043 - accuracy: 0.7871 - val_loss: 1.0644 - val_accuracy: 0.6267\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6214 - accuracy: 0.7769 - val_loss: 1.0228 - val_accuracy: 0.6267\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6104 - accuracy: 0.7895 - val_loss: 1.0254 - val_accuracy: 0.6388\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6218 - accuracy: 0.7721 - val_loss: 1.0321 - val_accuracy: 0.6279\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.6208 - accuracy: 0.7817 - val_loss: 1.0498 - val_accuracy: 0.6327\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6229 - accuracy: 0.7787 - val_loss: 1.0257 - val_accuracy: 0.6267\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6114 - accuracy: 0.7781 - val_loss: 1.0325 - val_accuracy: 0.6364\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.6169 - accuracy: 0.7733 - val_loss: 1.0187 - val_accuracy: 0.6424\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6116 - accuracy: 0.7961 - val_loss: 1.0226 - val_accuracy: 0.6376\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6151 - accuracy: 0.7763 - val_loss: 1.0248 - val_accuracy: 0.6339\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6129 - accuracy: 0.7841 - val_loss: 1.0146 - val_accuracy: 0.6291\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6182 - accuracy: 0.7691 - val_loss: 1.0018 - val_accuracy: 0.6255\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6151 - accuracy: 0.7847 - val_loss: 1.0204 - val_accuracy: 0.6291\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5995 - accuracy: 0.7877 - val_loss: 1.0374 - val_accuracy: 0.6436\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6032 - accuracy: 0.7841 - val_loss: 1.0486 - val_accuracy: 0.6412\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5931 - accuracy: 0.7925 - val_loss: 1.0220 - val_accuracy: 0.6376\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6116 - accuracy: 0.7829 - val_loss: 1.0023 - val_accuracy: 0.6291\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5788 - accuracy: 0.7978 - val_loss: 1.0325 - val_accuracy: 0.6182\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6042 - accuracy: 0.7829 - val_loss: 1.0096 - val_accuracy: 0.6400\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5980 - accuracy: 0.7931 - val_loss: 1.0121 - val_accuracy: 0.6473\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5915 - accuracy: 0.7829 - val_loss: 1.0355 - val_accuracy: 0.6291\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5958 - accuracy: 0.7817 - val_loss: 1.0238 - val_accuracy: 0.6339\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5913 - accuracy: 0.7949 - val_loss: 1.0386 - val_accuracy: 0.6352\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6075 - accuracy: 0.7727 - val_loss: 1.0104 - val_accuracy: 0.6315\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6020 - accuracy: 0.7823 - val_loss: 1.0438 - val_accuracy: 0.6267\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6001 - accuracy: 0.7913 - val_loss: 1.0185 - val_accuracy: 0.6255\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5953 - accuracy: 0.7889 - val_loss: 1.0371 - val_accuracy: 0.6170\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5944 - accuracy: 0.7907 - val_loss: 1.0431 - val_accuracy: 0.6376\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5960 - accuracy: 0.7829 - val_loss: 1.0162 - val_accuracy: 0.6400\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5912 - accuracy: 0.7931 - val_loss: 1.0229 - val_accuracy: 0.6388\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5865 - accuracy: 0.7996 - val_loss: 1.0391 - val_accuracy: 0.6497\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5855 - accuracy: 0.7978 - val_loss: 1.0170 - val_accuracy: 0.6339\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5912 - accuracy: 0.7907 - val_loss: 1.0155 - val_accuracy: 0.6400\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5726 - accuracy: 0.7972 - val_loss: 1.0476 - val_accuracy: 0.6339\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.6028 - accuracy: 0.7835 - val_loss: 1.0515 - val_accuracy: 0.6242\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5867 - accuracy: 0.7919 - val_loss: 1.0119 - val_accuracy: 0.6376\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5961 - accuracy: 0.7841 - val_loss: 1.0091 - val_accuracy: 0.6412\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5746 - accuracy: 0.7990 - val_loss: 1.0045 - val_accuracy: 0.6339\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5838 - accuracy: 0.7961 - val_loss: 0.9969 - val_accuracy: 0.6473\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5679 - accuracy: 0.8002 - val_loss: 1.0255 - val_accuracy: 0.6376\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5743 - accuracy: 0.7996 - val_loss: 1.0051 - val_accuracy: 0.6461\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5751 - accuracy: 0.7937 - val_loss: 1.0010 - val_accuracy: 0.6594\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5628 - accuracy: 0.8050 - val_loss: 1.0133 - val_accuracy: 0.6364\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5803 - accuracy: 0.7925 - val_loss: 1.0031 - val_accuracy: 0.6230\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5617 - accuracy: 0.7937 - val_loss: 1.0011 - val_accuracy: 0.6485\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5848 - accuracy: 0.7972 - val_loss: 1.0042 - val_accuracy: 0.6594\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5674 - accuracy: 0.8050 - val_loss: 1.0133 - val_accuracy: 0.6424\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5748 - accuracy: 0.8002 - val_loss: 1.0452 - val_accuracy: 0.6218\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5653 - accuracy: 0.8032 - val_loss: 1.0191 - val_accuracy: 0.6388\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5594 - accuracy: 0.8068 - val_loss: 1.0033 - val_accuracy: 0.6327\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5665 - accuracy: 0.7901 - val_loss: 0.9958 - val_accuracy: 0.6242\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5651 - accuracy: 0.7990 - val_loss: 1.0044 - val_accuracy: 0.6509\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5634 - accuracy: 0.8062 - val_loss: 1.0277 - val_accuracy: 0.6339\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5444 - accuracy: 0.7996 - val_loss: 1.0137 - val_accuracy: 0.6303\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5491 - accuracy: 0.8098 - val_loss: 1.0123 - val_accuracy: 0.6388\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5585 - accuracy: 0.8026 - val_loss: 1.0500 - val_accuracy: 0.6352\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5672 - accuracy: 0.7853 - val_loss: 1.0179 - val_accuracy: 0.6339\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5737 - accuracy: 0.8038 - val_loss: 1.0205 - val_accuracy: 0.6412\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5582 - accuracy: 0.7990 - val_loss: 1.0202 - val_accuracy: 0.6412\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5652 - accuracy: 0.7913 - val_loss: 1.0209 - val_accuracy: 0.6400\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5573 - accuracy: 0.8038 - val_loss: 1.0150 - val_accuracy: 0.6448\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5602 - accuracy: 0.8008 - val_loss: 1.0128 - val_accuracy: 0.6473\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5517 - accuracy: 0.8038 - val_loss: 1.0131 - val_accuracy: 0.6388\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5537 - accuracy: 0.7943 - val_loss: 1.0097 - val_accuracy: 0.6352\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5479 - accuracy: 0.8116 - val_loss: 1.0116 - val_accuracy: 0.6461\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5421 - accuracy: 0.8080 - val_loss: 1.0054 - val_accuracy: 0.6570\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5375 - accuracy: 0.8140 - val_loss: 0.9931 - val_accuracy: 0.6497\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5485 - accuracy: 0.8098 - val_loss: 1.0467 - val_accuracy: 0.6315\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5248 - accuracy: 0.8146 - val_loss: 1.0060 - val_accuracy: 0.6558\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5382 - accuracy: 0.8038 - val_loss: 1.0059 - val_accuracy: 0.6315\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5527 - accuracy: 0.8092 - val_loss: 1.0018 - val_accuracy: 0.6448\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5517 - accuracy: 0.7967 - val_loss: 0.9871 - val_accuracy: 0.6448\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5408 - accuracy: 0.7978 - val_loss: 1.0581 - val_accuracy: 0.6412\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5368 - accuracy: 0.8176 - val_loss: 1.0374 - val_accuracy: 0.6461\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5303 - accuracy: 0.8002 - val_loss: 1.0299 - val_accuracy: 0.6364\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5467 - accuracy: 0.7990 - val_loss: 1.0325 - val_accuracy: 0.6242\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5474 - accuracy: 0.8086 - val_loss: 0.9920 - val_accuracy: 0.6424\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5287 - accuracy: 0.8146 - val_loss: 1.0170 - val_accuracy: 0.6521\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5359 - accuracy: 0.8146 - val_loss: 1.0019 - val_accuracy: 0.6436\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5314 - accuracy: 0.8098 - val_loss: 0.9888 - val_accuracy: 0.6533\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5182 - accuracy: 0.8182 - val_loss: 0.9909 - val_accuracy: 0.6521\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5271 - accuracy: 0.8200 - val_loss: 1.0112 - val_accuracy: 0.6364\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5179 - accuracy: 0.8176 - val_loss: 1.0295 - val_accuracy: 0.6315\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5439 - accuracy: 0.8050 - val_loss: 1.0326 - val_accuracy: 0.6279\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5292 - accuracy: 0.8182 - val_loss: 1.0159 - val_accuracy: 0.6436\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5309 - accuracy: 0.8212 - val_loss: 1.0022 - val_accuracy: 0.6485\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5321 - accuracy: 0.8176 - val_loss: 1.0042 - val_accuracy: 0.6327\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5187 - accuracy: 0.8122 - val_loss: 1.0205 - val_accuracy: 0.6448\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5195 - accuracy: 0.8122 - val_loss: 1.0023 - val_accuracy: 0.6424\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5257 - accuracy: 0.8164 - val_loss: 1.0032 - val_accuracy: 0.6570\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5169 - accuracy: 0.8176 - val_loss: 1.0128 - val_accuracy: 0.6485\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5069 - accuracy: 0.8218 - val_loss: 1.0220 - val_accuracy: 0.6424\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5098 - accuracy: 0.8248 - val_loss: 1.0051 - val_accuracy: 0.6545\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5018 - accuracy: 0.8194 - val_loss: 1.0120 - val_accuracy: 0.6448\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5283 - accuracy: 0.8200 - val_loss: 0.9913 - val_accuracy: 0.6424\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5144 - accuracy: 0.8134 - val_loss: 1.0170 - val_accuracy: 0.6352\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5404 - accuracy: 0.8092 - val_loss: 1.0401 - val_accuracy: 0.6352\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5279 - accuracy: 0.8164 - val_loss: 0.9912 - val_accuracy: 0.6327\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5122 - accuracy: 0.8152 - val_loss: 1.0039 - val_accuracy: 0.6303\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5191 - accuracy: 0.8074 - val_loss: 1.0275 - val_accuracy: 0.6509\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5173 - accuracy: 0.8182 - val_loss: 1.0072 - val_accuracy: 0.6436\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5197 - accuracy: 0.8200 - val_loss: 0.9998 - val_accuracy: 0.6533\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.5176 - accuracy: 0.8152 - val_loss: 1.0371 - val_accuracy: 0.6315\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5150 - accuracy: 0.8164 - val_loss: 1.0552 - val_accuracy: 0.6315\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5168 - accuracy: 0.8116 - val_loss: 1.0083 - val_accuracy: 0.6412\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.5062 - accuracy: 0.8260 - val_loss: 1.0475 - val_accuracy: 0.6352\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4972 - accuracy: 0.8254 - val_loss: 1.0171 - val_accuracy: 0.6497\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5150 - accuracy: 0.8158 - val_loss: 1.0247 - val_accuracy: 0.6473\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5068 - accuracy: 0.8152 - val_loss: 1.0375 - val_accuracy: 0.6376\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.4943 - accuracy: 0.8194 - val_loss: 0.9987 - val_accuracy: 0.6424\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4991 - accuracy: 0.8128 - val_loss: 1.0210 - val_accuracy: 0.6606\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4994 - accuracy: 0.8200 - val_loss: 1.0015 - val_accuracy: 0.6521\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5028 - accuracy: 0.8236 - val_loss: 1.0076 - val_accuracy: 0.6485\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4932 - accuracy: 0.8194 - val_loss: 1.0211 - val_accuracy: 0.6594\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4879 - accuracy: 0.8349 - val_loss: 0.9981 - val_accuracy: 0.6594\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4939 - accuracy: 0.8224 - val_loss: 1.0146 - val_accuracy: 0.6461\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4951 - accuracy: 0.8218 - val_loss: 1.0383 - val_accuracy: 0.6509\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4911 - accuracy: 0.8248 - val_loss: 1.0157 - val_accuracy: 0.6448\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4827 - accuracy: 0.8379 - val_loss: 1.0009 - val_accuracy: 0.6448\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4943 - accuracy: 0.8206 - val_loss: 1.0071 - val_accuracy: 0.6582\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5071 - accuracy: 0.8230 - val_loss: 1.0081 - val_accuracy: 0.6376\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.5189 - accuracy: 0.8224 - val_loss: 1.0046 - val_accuracy: 0.6436\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4834 - accuracy: 0.8343 - val_loss: 1.0142 - val_accuracy: 0.6485\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4949 - accuracy: 0.8337 - val_loss: 1.0344 - val_accuracy: 0.6545\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.5027 - accuracy: 0.8194 - val_loss: 0.9980 - val_accuracy: 0.6630\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4890 - accuracy: 0.8289 - val_loss: 1.0348 - val_accuracy: 0.6545\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4958 - accuracy: 0.8325 - val_loss: 1.0030 - val_accuracy: 0.6521\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4942 - accuracy: 0.8248 - val_loss: 1.0186 - val_accuracy: 0.6473\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4667 - accuracy: 0.8415 - val_loss: 1.0330 - val_accuracy: 0.6461\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4935 - accuracy: 0.8313 - val_loss: 1.0103 - val_accuracy: 0.6473\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4958 - accuracy: 0.8278 - val_loss: 1.0071 - val_accuracy: 0.6461\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4771 - accuracy: 0.8349 - val_loss: 0.9998 - val_accuracy: 0.6436\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4660 - accuracy: 0.8313 - val_loss: 1.0232 - val_accuracy: 0.6279\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4806 - accuracy: 0.8230 - val_loss: 0.9890 - val_accuracy: 0.6436\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4738 - accuracy: 0.8391 - val_loss: 1.0743 - val_accuracy: 0.6448\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4761 - accuracy: 0.8283 - val_loss: 1.0075 - val_accuracy: 0.6497\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4698 - accuracy: 0.8254 - val_loss: 0.9991 - val_accuracy: 0.6461\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4717 - accuracy: 0.8343 - val_loss: 1.0165 - val_accuracy: 0.6521\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 1s 8ms/step - loss: 0.4675 - accuracy: 0.8397 - val_loss: 1.0186 - val_accuracy: 0.6521\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4706 - accuracy: 0.8481 - val_loss: 1.0031 - val_accuracy: 0.6594\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4759 - accuracy: 0.8397 - val_loss: 1.0019 - val_accuracy: 0.6642\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4727 - accuracy: 0.8301 - val_loss: 1.0220 - val_accuracy: 0.6582\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4561 - accuracy: 0.8511 - val_loss: 1.0198 - val_accuracy: 0.6473\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4717 - accuracy: 0.8331 - val_loss: 1.0041 - val_accuracy: 0.6655\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4600 - accuracy: 0.8272 - val_loss: 0.9993 - val_accuracy: 0.6533\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4554 - accuracy: 0.8415 - val_loss: 1.0391 - val_accuracy: 0.6448\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4930 - accuracy: 0.8218 - val_loss: 1.0284 - val_accuracy: 0.6448\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4663 - accuracy: 0.8385 - val_loss: 1.0037 - val_accuracy: 0.6497\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4717 - accuracy: 0.8367 - val_loss: 1.0073 - val_accuracy: 0.6558\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4829 - accuracy: 0.8278 - val_loss: 1.0063 - val_accuracy: 0.6558\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4668 - accuracy: 0.8313 - val_loss: 0.9802 - val_accuracy: 0.6594\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4797 - accuracy: 0.8301 - val_loss: 0.9907 - val_accuracy: 0.6533\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4650 - accuracy: 0.8409 - val_loss: 0.9917 - val_accuracy: 0.6618\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4730 - accuracy: 0.8236 - val_loss: 1.0044 - val_accuracy: 0.6497\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4673 - accuracy: 0.8295 - val_loss: 1.0473 - val_accuracy: 0.6461\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4644 - accuracy: 0.8343 - val_loss: 1.0281 - val_accuracy: 0.6388\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4676 - accuracy: 0.8343 - val_loss: 1.0182 - val_accuracy: 0.6533\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4736 - accuracy: 0.8421 - val_loss: 0.9870 - val_accuracy: 0.6630\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4627 - accuracy: 0.8361 - val_loss: 1.0115 - val_accuracy: 0.6715\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4517 - accuracy: 0.8403 - val_loss: 0.9855 - val_accuracy: 0.6642\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4580 - accuracy: 0.8307 - val_loss: 1.0320 - val_accuracy: 0.6485\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4623 - accuracy: 0.8427 - val_loss: 1.0011 - val_accuracy: 0.6642\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4506 - accuracy: 0.8373 - val_loss: 0.9882 - val_accuracy: 0.6642\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4730 - accuracy: 0.8325 - val_loss: 0.9900 - val_accuracy: 0.6509\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4429 - accuracy: 0.8445 - val_loss: 1.0182 - val_accuracy: 0.6545\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4306 - accuracy: 0.8559 - val_loss: 1.0380 - val_accuracy: 0.6533\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4389 - accuracy: 0.8469 - val_loss: 1.0060 - val_accuracy: 0.6412\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4583 - accuracy: 0.8409 - val_loss: 1.0408 - val_accuracy: 0.6533\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4358 - accuracy: 0.8499 - val_loss: 1.0018 - val_accuracy: 0.6606\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4438 - accuracy: 0.8379 - val_loss: 1.0201 - val_accuracy: 0.6497\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4562 - accuracy: 0.8457 - val_loss: 0.9893 - val_accuracy: 0.6509\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4606 - accuracy: 0.8343 - val_loss: 1.0114 - val_accuracy: 0.6545\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4416 - accuracy: 0.8541 - val_loss: 1.0278 - val_accuracy: 0.6412\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4392 - accuracy: 0.8421 - val_loss: 1.0050 - val_accuracy: 0.6655\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4517 - accuracy: 0.8391 - val_loss: 1.0063 - val_accuracy: 0.6630\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4432 - accuracy: 0.8511 - val_loss: 1.0012 - val_accuracy: 0.6594\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4331 - accuracy: 0.8571 - val_loss: 1.0165 - val_accuracy: 0.6558\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4388 - accuracy: 0.8469 - val_loss: 0.9926 - val_accuracy: 0.6497\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4485 - accuracy: 0.8505 - val_loss: 0.9870 - val_accuracy: 0.6727\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4355 - accuracy: 0.8499 - val_loss: 1.0001 - val_accuracy: 0.6533\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4497 - accuracy: 0.8421 - val_loss: 1.0310 - val_accuracy: 0.6400\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4388 - accuracy: 0.8415 - val_loss: 1.0461 - val_accuracy: 0.6388\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4332 - accuracy: 0.8475 - val_loss: 1.0205 - val_accuracy: 0.6473\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4248 - accuracy: 0.8565 - val_loss: 1.0054 - val_accuracy: 0.6642\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4274 - accuracy: 0.8493 - val_loss: 1.0082 - val_accuracy: 0.6594\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4341 - accuracy: 0.8553 - val_loss: 1.0203 - val_accuracy: 0.6606\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4302 - accuracy: 0.8541 - val_loss: 1.0641 - val_accuracy: 0.6570\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4199 - accuracy: 0.8475 - val_loss: 1.0833 - val_accuracy: 0.6497\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4312 - accuracy: 0.8499 - val_loss: 1.0188 - val_accuracy: 0.6521\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4257 - accuracy: 0.8463 - val_loss: 1.0061 - val_accuracy: 0.6509\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4273 - accuracy: 0.8571 - val_loss: 1.0091 - val_accuracy: 0.6642\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4039 - accuracy: 0.8666 - val_loss: 1.0239 - val_accuracy: 0.6570\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4273 - accuracy: 0.8451 - val_loss: 1.0261 - val_accuracy: 0.6715\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4199 - accuracy: 0.8529 - val_loss: 0.9922 - val_accuracy: 0.6485\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4190 - accuracy: 0.8475 - val_loss: 1.0207 - val_accuracy: 0.6667\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4310 - accuracy: 0.8421 - val_loss: 1.0176 - val_accuracy: 0.6655\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4339 - accuracy: 0.8415 - val_loss: 1.0069 - val_accuracy: 0.6448\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4211 - accuracy: 0.8469 - val_loss: 1.0092 - val_accuracy: 0.6558\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4158 - accuracy: 0.8583 - val_loss: 1.0161 - val_accuracy: 0.6570\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4154 - accuracy: 0.8565 - val_loss: 1.0143 - val_accuracy: 0.6545\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4146 - accuracy: 0.8636 - val_loss: 0.9966 - val_accuracy: 0.6485\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4272 - accuracy: 0.8475 - val_loss: 1.0287 - val_accuracy: 0.6509\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4377 - accuracy: 0.8415 - val_loss: 1.0195 - val_accuracy: 0.6630\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4089 - accuracy: 0.8577 - val_loss: 1.0044 - val_accuracy: 0.6582\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4300 - accuracy: 0.8463 - val_loss: 1.0189 - val_accuracy: 0.6606\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4020 - accuracy: 0.8708 - val_loss: 1.0140 - val_accuracy: 0.6679\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4151 - accuracy: 0.8517 - val_loss: 1.0196 - val_accuracy: 0.6655\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4242 - accuracy: 0.8589 - val_loss: 1.0367 - val_accuracy: 0.6691\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3906 - accuracy: 0.8720 - val_loss: 1.0554 - val_accuracy: 0.6582\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4207 - accuracy: 0.8594 - val_loss: 1.0477 - val_accuracy: 0.6582\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4035 - accuracy: 0.8553 - val_loss: 1.0079 - val_accuracy: 0.6642\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4188 - accuracy: 0.8481 - val_loss: 1.0578 - val_accuracy: 0.6727\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4163 - accuracy: 0.8475 - val_loss: 1.0189 - val_accuracy: 0.6727\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4125 - accuracy: 0.8606 - val_loss: 1.0101 - val_accuracy: 0.6630\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4336 - accuracy: 0.8475 - val_loss: 1.0194 - val_accuracy: 0.6655\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4015 - accuracy: 0.8636 - val_loss: 1.0158 - val_accuracy: 0.6570\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4064 - accuracy: 0.8606 - val_loss: 1.0083 - val_accuracy: 0.6655\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4094 - accuracy: 0.8630 - val_loss: 1.0172 - val_accuracy: 0.6752\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4108 - accuracy: 0.8553 - val_loss: 1.0286 - val_accuracy: 0.6582\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4360 - accuracy: 0.8361 - val_loss: 1.0135 - val_accuracy: 0.6558\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3922 - accuracy: 0.8612 - val_loss: 1.0367 - val_accuracy: 0.6570\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4158 - accuracy: 0.8505 - val_loss: 1.0336 - val_accuracy: 0.6618\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3912 - accuracy: 0.8690 - val_loss: 1.0459 - val_accuracy: 0.6509\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3868 - accuracy: 0.8654 - val_loss: 1.0394 - val_accuracy: 0.6594\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3893 - accuracy: 0.8636 - val_loss: 1.0269 - val_accuracy: 0.6642\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4094 - accuracy: 0.8630 - val_loss: 1.0149 - val_accuracy: 0.6606\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4113 - accuracy: 0.8583 - val_loss: 1.0009 - val_accuracy: 0.6594\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4043 - accuracy: 0.8571 - val_loss: 1.0361 - val_accuracy: 0.6509\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4094 - accuracy: 0.8463 - val_loss: 0.9948 - val_accuracy: 0.6642\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3977 - accuracy: 0.8684 - val_loss: 1.0194 - val_accuracy: 0.6558\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3817 - accuracy: 0.8708 - val_loss: 1.0447 - val_accuracy: 0.6630\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3879 - accuracy: 0.8589 - val_loss: 1.0491 - val_accuracy: 0.6570\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3854 - accuracy: 0.8630 - val_loss: 1.0177 - val_accuracy: 0.6618\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3876 - accuracy: 0.8720 - val_loss: 0.9957 - val_accuracy: 0.6727\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4106 - accuracy: 0.8559 - val_loss: 1.0242 - val_accuracy: 0.6533\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3921 - accuracy: 0.8624 - val_loss: 1.0456 - val_accuracy: 0.6582\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3888 - accuracy: 0.8684 - val_loss: 0.9975 - val_accuracy: 0.6606\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3824 - accuracy: 0.8714 - val_loss: 0.9979 - val_accuracy: 0.6642\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4016 - accuracy: 0.8547 - val_loss: 0.9892 - val_accuracy: 0.6606\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.4010 - accuracy: 0.8571 - val_loss: 1.0281 - val_accuracy: 0.6642\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3889 - accuracy: 0.8648 - val_loss: 1.0107 - val_accuracy: 0.6739\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3849 - accuracy: 0.8684 - val_loss: 1.0391 - val_accuracy: 0.6594\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.4153 - accuracy: 0.8565 - val_loss: 1.0104 - val_accuracy: 0.6752\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3770 - accuracy: 0.8762 - val_loss: 0.9957 - val_accuracy: 0.6679\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3832 - accuracy: 0.8648 - val_loss: 0.9931 - val_accuracy: 0.6667\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3993 - accuracy: 0.8577 - val_loss: 1.0077 - val_accuracy: 0.6679\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3831 - accuracy: 0.8726 - val_loss: 1.0110 - val_accuracy: 0.6497\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3812 - accuracy: 0.8696 - val_loss: 1.0169 - val_accuracy: 0.6727\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3904 - accuracy: 0.8565 - val_loss: 1.0180 - val_accuracy: 0.6703\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3894 - accuracy: 0.8696 - val_loss: 1.0254 - val_accuracy: 0.6679\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3677 - accuracy: 0.8792 - val_loss: 1.0342 - val_accuracy: 0.6558\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3838 - accuracy: 0.8642 - val_loss: 1.0317 - val_accuracy: 0.6630\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3873 - accuracy: 0.8600 - val_loss: 1.0313 - val_accuracy: 0.6642\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3558 - accuracy: 0.8738 - val_loss: 1.0303 - val_accuracy: 0.6715\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3935 - accuracy: 0.8708 - val_loss: 1.0086 - val_accuracy: 0.6655\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3734 - accuracy: 0.8666 - val_loss: 1.0284 - val_accuracy: 0.6739\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3635 - accuracy: 0.8804 - val_loss: 1.0302 - val_accuracy: 0.6703\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3705 - accuracy: 0.8756 - val_loss: 1.0203 - val_accuracy: 0.6679\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3691 - accuracy: 0.8678 - val_loss: 1.0273 - val_accuracy: 0.6715\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3839 - accuracy: 0.8714 - val_loss: 1.0248 - val_accuracy: 0.6667\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3606 - accuracy: 0.8756 - val_loss: 1.0204 - val_accuracy: 0.6679\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3879 - accuracy: 0.8702 - val_loss: 1.0319 - val_accuracy: 0.6606\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3787 - accuracy: 0.8798 - val_loss: 1.0208 - val_accuracy: 0.6630\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3757 - accuracy: 0.8732 - val_loss: 0.9997 - val_accuracy: 0.6739\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3552 - accuracy: 0.8762 - val_loss: 1.0416 - val_accuracy: 0.6655\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3674 - accuracy: 0.8756 - val_loss: 1.0578 - val_accuracy: 0.6606\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3574 - accuracy: 0.8798 - val_loss: 1.0333 - val_accuracy: 0.6679\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3668 - accuracy: 0.8774 - val_loss: 1.0158 - val_accuracy: 0.6558\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3722 - accuracy: 0.8648 - val_loss: 1.0180 - val_accuracy: 0.6679\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3704 - accuracy: 0.8708 - val_loss: 1.0355 - val_accuracy: 0.6545\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3822 - accuracy: 0.8666 - val_loss: 1.0097 - val_accuracy: 0.6667\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3729 - accuracy: 0.8642 - val_loss: 1.0083 - val_accuracy: 0.6655\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3526 - accuracy: 0.8804 - val_loss: 1.0414 - val_accuracy: 0.6533\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3666 - accuracy: 0.8732 - val_loss: 1.0250 - val_accuracy: 0.6667\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3569 - accuracy: 0.8906 - val_loss: 1.0449 - val_accuracy: 0.6655\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3597 - accuracy: 0.8756 - val_loss: 1.0011 - val_accuracy: 0.6642\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3629 - accuracy: 0.8804 - val_loss: 1.0388 - val_accuracy: 0.6606\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3493 - accuracy: 0.8876 - val_loss: 1.0085 - val_accuracy: 0.6727\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3705 - accuracy: 0.8738 - val_loss: 1.0166 - val_accuracy: 0.6739\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3499 - accuracy: 0.8774 - val_loss: 1.0323 - val_accuracy: 0.6764\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3477 - accuracy: 0.8822 - val_loss: 1.0320 - val_accuracy: 0.6703\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3585 - accuracy: 0.8768 - val_loss: 1.0397 - val_accuracy: 0.6606\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3691 - accuracy: 0.8774 - val_loss: 1.0132 - val_accuracy: 0.6703\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3563 - accuracy: 0.8738 - val_loss: 1.0167 - val_accuracy: 0.6703\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3636 - accuracy: 0.8672 - val_loss: 1.0405 - val_accuracy: 0.6691\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3650 - accuracy: 0.8804 - val_loss: 1.0105 - val_accuracy: 0.6752\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3602 - accuracy: 0.8732 - val_loss: 1.0057 - val_accuracy: 0.6764\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3594 - accuracy: 0.8786 - val_loss: 1.0679 - val_accuracy: 0.6545\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3499 - accuracy: 0.8852 - val_loss: 1.0234 - val_accuracy: 0.6606\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3616 - accuracy: 0.8762 - val_loss: 1.0365 - val_accuracy: 0.6655\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3500 - accuracy: 0.8840 - val_loss: 1.0218 - val_accuracy: 0.6630\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3616 - accuracy: 0.8804 - val_loss: 1.0091 - val_accuracy: 0.6764\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3644 - accuracy: 0.8744 - val_loss: 1.0398 - val_accuracy: 0.6618\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3411 - accuracy: 0.8828 - val_loss: 1.0190 - val_accuracy: 0.6582\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3474 - accuracy: 0.8768 - val_loss: 1.0243 - val_accuracy: 0.6776\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3573 - accuracy: 0.8780 - val_loss: 1.0476 - val_accuracy: 0.6764\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3524 - accuracy: 0.8774 - val_loss: 1.0192 - val_accuracy: 0.6655\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3574 - accuracy: 0.8780 - val_loss: 1.0289 - val_accuracy: 0.6703\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3503 - accuracy: 0.8738 - val_loss: 1.0063 - val_accuracy: 0.6715\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3364 - accuracy: 0.8792 - val_loss: 1.0124 - val_accuracy: 0.6691\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3527 - accuracy: 0.8858 - val_loss: 1.0498 - val_accuracy: 0.6630\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3390 - accuracy: 0.8780 - val_loss: 1.0000 - val_accuracy: 0.6703\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3448 - accuracy: 0.8900 - val_loss: 1.0128 - val_accuracy: 0.6752\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3678 - accuracy: 0.8708 - val_loss: 1.0253 - val_accuracy: 0.6655\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3444 - accuracy: 0.8876 - val_loss: 1.0228 - val_accuracy: 0.6752\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3309 - accuracy: 0.8917 - val_loss: 1.0264 - val_accuracy: 0.6703\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3686 - accuracy: 0.8696 - val_loss: 1.0512 - val_accuracy: 0.6606\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3512 - accuracy: 0.8828 - val_loss: 1.0237 - val_accuracy: 0.6618\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3297 - accuracy: 0.8888 - val_loss: 1.0413 - val_accuracy: 0.6703\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3355 - accuracy: 0.8846 - val_loss: 1.0375 - val_accuracy: 0.6715\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3257 - accuracy: 0.8917 - val_loss: 1.0320 - val_accuracy: 0.6861\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3538 - accuracy: 0.8774 - val_loss: 1.0439 - val_accuracy: 0.6667\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3298 - accuracy: 0.8858 - val_loss: 1.0396 - val_accuracy: 0.6655\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3501 - accuracy: 0.8678 - val_loss: 1.0376 - val_accuracy: 0.6727\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3374 - accuracy: 0.8786 - val_loss: 1.0128 - val_accuracy: 0.6691\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3561 - accuracy: 0.8684 - val_loss: 1.0271 - val_accuracy: 0.6715\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3287 - accuracy: 0.8840 - val_loss: 1.0454 - val_accuracy: 0.6752\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3331 - accuracy: 0.8858 - val_loss: 1.0177 - val_accuracy: 0.6836\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3507 - accuracy: 0.8840 - val_loss: 1.0225 - val_accuracy: 0.6764\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3177 - accuracy: 0.8959 - val_loss: 1.0299 - val_accuracy: 0.6800\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3563 - accuracy: 0.8834 - val_loss: 1.0231 - val_accuracy: 0.6655\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3302 - accuracy: 0.8917 - val_loss: 1.0069 - val_accuracy: 0.6655\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3294 - accuracy: 0.8923 - val_loss: 1.0127 - val_accuracy: 0.6606\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3309 - accuracy: 0.8846 - val_loss: 1.0188 - val_accuracy: 0.6752\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3407 - accuracy: 0.8804 - val_loss: 1.0369 - val_accuracy: 0.6812\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3512 - accuracy: 0.8822 - val_loss: 1.0533 - val_accuracy: 0.6655\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3325 - accuracy: 0.8870 - val_loss: 1.0103 - val_accuracy: 0.6655\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3541 - accuracy: 0.8750 - val_loss: 1.0053 - val_accuracy: 0.6764\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3256 - accuracy: 0.8888 - val_loss: 1.0208 - val_accuracy: 0.6642\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3300 - accuracy: 0.8846 - val_loss: 1.0246 - val_accuracy: 0.6727\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3408 - accuracy: 0.8804 - val_loss: 1.0391 - val_accuracy: 0.6667\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3353 - accuracy: 0.8810 - val_loss: 1.0269 - val_accuracy: 0.6752\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3321 - accuracy: 0.8882 - val_loss: 1.0036 - val_accuracy: 0.6824\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3105 - accuracy: 0.9007 - val_loss: 1.0202 - val_accuracy: 0.6667\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3091 - accuracy: 0.8977 - val_loss: 1.0326 - val_accuracy: 0.6618\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3268 - accuracy: 0.8947 - val_loss: 1.0634 - val_accuracy: 0.6715\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3068 - accuracy: 0.9013 - val_loss: 1.0931 - val_accuracy: 0.6655\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3198 - accuracy: 0.8935 - val_loss: 1.0114 - val_accuracy: 0.6679\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3330 - accuracy: 0.8858 - val_loss: 1.0532 - val_accuracy: 0.6667\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3325 - accuracy: 0.8834 - val_loss: 1.0155 - val_accuracy: 0.6703\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3279 - accuracy: 0.8977 - val_loss: 1.0359 - val_accuracy: 0.6642\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3204 - accuracy: 0.8929 - val_loss: 1.0418 - val_accuracy: 0.6655\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3311 - accuracy: 0.8852 - val_loss: 1.0452 - val_accuracy: 0.6776\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3229 - accuracy: 0.8906 - val_loss: 1.0472 - val_accuracy: 0.6703\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3230 - accuracy: 0.8888 - val_loss: 1.0184 - val_accuracy: 0.6703\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3122 - accuracy: 0.8935 - val_loss: 1.0276 - val_accuracy: 0.6703\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3120 - accuracy: 0.9001 - val_loss: 1.0264 - val_accuracy: 0.6739\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3158 - accuracy: 0.8947 - val_loss: 1.0123 - val_accuracy: 0.6679\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3209 - accuracy: 0.8900 - val_loss: 1.0298 - val_accuracy: 0.6739\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3127 - accuracy: 0.8965 - val_loss: 1.0311 - val_accuracy: 0.6667\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3104 - accuracy: 0.8983 - val_loss: 1.0578 - val_accuracy: 0.6618\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3060 - accuracy: 0.8858 - val_loss: 1.0354 - val_accuracy: 0.6824\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3406 - accuracy: 0.8876 - val_loss: 1.0279 - val_accuracy: 0.6667\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3204 - accuracy: 0.8894 - val_loss: 1.0180 - val_accuracy: 0.6642\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3152 - accuracy: 0.8917 - val_loss: 1.0309 - val_accuracy: 0.6642\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3105 - accuracy: 0.8989 - val_loss: 1.0153 - val_accuracy: 0.6776\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3136 - accuracy: 0.8882 - val_loss: 1.0430 - val_accuracy: 0.6606\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2893 - accuracy: 0.9073 - val_loss: 1.0581 - val_accuracy: 0.6715\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3080 - accuracy: 0.8947 - val_loss: 1.0586 - val_accuracy: 0.6715\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3116 - accuracy: 0.8917 - val_loss: 1.0440 - val_accuracy: 0.6691\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3267 - accuracy: 0.8888 - val_loss: 1.0684 - val_accuracy: 0.6752\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3210 - accuracy: 0.8923 - val_loss: 1.0225 - val_accuracy: 0.6812\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3273 - accuracy: 0.8816 - val_loss: 1.0824 - val_accuracy: 0.6776\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3008 - accuracy: 0.9037 - val_loss: 1.0587 - val_accuracy: 0.6630\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3197 - accuracy: 0.8941 - val_loss: 1.0470 - val_accuracy: 0.6667\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3075 - accuracy: 0.8971 - val_loss: 1.0093 - val_accuracy: 0.6715\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3067 - accuracy: 0.8965 - val_loss: 1.0419 - val_accuracy: 0.6764\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3063 - accuracy: 0.8959 - val_loss: 1.0159 - val_accuracy: 0.6691\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3238 - accuracy: 0.8894 - val_loss: 1.0415 - val_accuracy: 0.6812\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2992 - accuracy: 0.8882 - val_loss: 1.0640 - val_accuracy: 0.6642\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3062 - accuracy: 0.8906 - val_loss: 1.0285 - val_accuracy: 0.6800\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3129 - accuracy: 0.8971 - val_loss: 1.0633 - val_accuracy: 0.6824\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3022 - accuracy: 0.8965 - val_loss: 1.0510 - val_accuracy: 0.6788\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3066 - accuracy: 0.8959 - val_loss: 1.0600 - val_accuracy: 0.6873\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2986 - accuracy: 0.8941 - val_loss: 1.0241 - val_accuracy: 0.6679\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3167 - accuracy: 0.8906 - val_loss: 1.0332 - val_accuracy: 0.6812\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2936 - accuracy: 0.8989 - val_loss: 1.0347 - val_accuracy: 0.6715\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3076 - accuracy: 0.8953 - val_loss: 1.0285 - val_accuracy: 0.6824\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3048 - accuracy: 0.8900 - val_loss: 1.0272 - val_accuracy: 0.6655\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2954 - accuracy: 0.8953 - val_loss: 1.0676 - val_accuracy: 0.6800\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3008 - accuracy: 0.9049 - val_loss: 1.0545 - val_accuracy: 0.6788\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.2985 - accuracy: 0.8983 - val_loss: 1.0396 - val_accuracy: 0.6824\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2791 - accuracy: 0.9055 - val_loss: 1.0850 - val_accuracy: 0.6752\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2857 - accuracy: 0.8965 - val_loss: 1.0424 - val_accuracy: 0.6727\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3084 - accuracy: 0.8900 - val_loss: 1.0703 - val_accuracy: 0.6715\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.2864 - accuracy: 0.9079 - val_loss: 1.0723 - val_accuracy: 0.6800\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3007 - accuracy: 0.8935 - val_loss: 1.0449 - val_accuracy: 0.6618\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2905 - accuracy: 0.9061 - val_loss: 1.0557 - val_accuracy: 0.6727\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2863 - accuracy: 0.9031 - val_loss: 1.0463 - val_accuracy: 0.6764\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2967 - accuracy: 0.9049 - val_loss: 1.0312 - val_accuracy: 0.6800\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3097 - accuracy: 0.8906 - val_loss: 1.0547 - val_accuracy: 0.6776\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3067 - accuracy: 0.8971 - val_loss: 1.0620 - val_accuracy: 0.6703\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2899 - accuracy: 0.8989 - val_loss: 1.0355 - val_accuracy: 0.6800\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2854 - accuracy: 0.9037 - val_loss: 1.0313 - val_accuracy: 0.6776\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2916 - accuracy: 0.9091 - val_loss: 1.0523 - val_accuracy: 0.6861\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2900 - accuracy: 0.8959 - val_loss: 1.0785 - val_accuracy: 0.6764\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3021 - accuracy: 0.8870 - val_loss: 1.0872 - val_accuracy: 0.6727\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.3093 - accuracy: 0.8953 - val_loss: 1.0557 - val_accuracy: 0.6715\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3015 - accuracy: 0.8935 - val_loss: 1.0335 - val_accuracy: 0.6606\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3000 - accuracy: 0.9007 - val_loss: 1.0519 - val_accuracy: 0.6764\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2784 - accuracy: 0.9031 - val_loss: 1.0511 - val_accuracy: 0.6715\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2861 - accuracy: 0.8917 - val_loss: 1.0819 - val_accuracy: 0.6788\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3020 - accuracy: 0.8906 - val_loss: 1.0834 - val_accuracy: 0.6848\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2897 - accuracy: 0.9007 - val_loss: 1.0513 - val_accuracy: 0.6715\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2850 - accuracy: 0.9013 - val_loss: 1.0570 - val_accuracy: 0.6727\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3013 - accuracy: 0.8959 - val_loss: 1.0649 - val_accuracy: 0.6667\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2991 - accuracy: 0.8959 - val_loss: 1.0500 - val_accuracy: 0.6727\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2966 - accuracy: 0.9001 - val_loss: 1.0839 - val_accuracy: 0.6788\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2885 - accuracy: 0.9061 - val_loss: 1.0756 - val_accuracy: 0.6776\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2947 - accuracy: 0.9037 - val_loss: 1.0347 - val_accuracy: 0.6727\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2702 - accuracy: 0.9037 - val_loss: 1.0639 - val_accuracy: 0.6885\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2923 - accuracy: 0.8947 - val_loss: 1.0465 - val_accuracy: 0.6776\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2995 - accuracy: 0.8977 - val_loss: 1.0663 - val_accuracy: 0.6848\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2863 - accuracy: 0.9007 - val_loss: 1.0629 - val_accuracy: 0.6800\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2981 - accuracy: 0.8917 - val_loss: 1.0384 - val_accuracy: 0.6776\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.3030 - accuracy: 0.9031 - val_loss: 1.0672 - val_accuracy: 0.6703\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2870 - accuracy: 0.9019 - val_loss: 1.0647 - val_accuracy: 0.6812\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2817 - accuracy: 0.9031 - val_loss: 1.0527 - val_accuracy: 0.6776\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2847 - accuracy: 0.8995 - val_loss: 1.0739 - val_accuracy: 0.6679\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2898 - accuracy: 0.8947 - val_loss: 1.0705 - val_accuracy: 0.6752\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2786 - accuracy: 0.9157 - val_loss: 1.1401 - val_accuracy: 0.6582\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2793 - accuracy: 0.9019 - val_loss: 1.0504 - val_accuracy: 0.6800\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2813 - accuracy: 0.9019 - val_loss: 1.0754 - val_accuracy: 0.6885\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2705 - accuracy: 0.9031 - val_loss: 1.0425 - val_accuracy: 0.6776\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2992 - accuracy: 0.8953 - val_loss: 1.0250 - val_accuracy: 0.6800\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2673 - accuracy: 0.9163 - val_loss: 1.0873 - val_accuracy: 0.6764\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2870 - accuracy: 0.9031 - val_loss: 1.0447 - val_accuracy: 0.6800\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2737 - accuracy: 0.9145 - val_loss: 1.0548 - val_accuracy: 0.6812\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2622 - accuracy: 0.9085 - val_loss: 1.0680 - val_accuracy: 0.6836\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2803 - accuracy: 0.9091 - val_loss: 1.0552 - val_accuracy: 0.6861\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2780 - accuracy: 0.9067 - val_loss: 1.0437 - val_accuracy: 0.6836\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2701 - accuracy: 0.9031 - val_loss: 1.0660 - val_accuracy: 0.6800\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2827 - accuracy: 0.9079 - val_loss: 1.0585 - val_accuracy: 0.6788\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2735 - accuracy: 0.9115 - val_loss: 1.0446 - val_accuracy: 0.6897\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2792 - accuracy: 0.9037 - val_loss: 1.0411 - val_accuracy: 0.6873\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2775 - accuracy: 0.9037 - val_loss: 1.0659 - val_accuracy: 0.6800\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2796 - accuracy: 0.9049 - val_loss: 1.0806 - val_accuracy: 0.6812\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2790 - accuracy: 0.9049 - val_loss: 1.0681 - val_accuracy: 0.6788\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2572 - accuracy: 0.9205 - val_loss: 1.0446 - val_accuracy: 0.6752\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2563 - accuracy: 0.9037 - val_loss: 1.0674 - val_accuracy: 0.6788\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2721 - accuracy: 0.9037 - val_loss: 1.0647 - val_accuracy: 0.6812\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2506 - accuracy: 0.9199 - val_loss: 1.0679 - val_accuracy: 0.6703\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2710 - accuracy: 0.9139 - val_loss: 1.0963 - val_accuracy: 0.6727\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2681 - accuracy: 0.9139 - val_loss: 1.0577 - val_accuracy: 0.6897\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2501 - accuracy: 0.9151 - val_loss: 1.0608 - val_accuracy: 0.6873\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2608 - accuracy: 0.9109 - val_loss: 1.0582 - val_accuracy: 0.6885\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2701 - accuracy: 0.9061 - val_loss: 1.0623 - val_accuracy: 0.6836\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2787 - accuracy: 0.9067 - val_loss: 1.0461 - val_accuracy: 0.6739\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2571 - accuracy: 0.9055 - val_loss: 1.0578 - val_accuracy: 0.6764\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2600 - accuracy: 0.9151 - val_loss: 1.0946 - val_accuracy: 0.6873\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2675 - accuracy: 0.9115 - val_loss: 1.0770 - val_accuracy: 0.6764\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2730 - accuracy: 0.9073 - val_loss: 1.0577 - val_accuracy: 0.6727\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2778 - accuracy: 0.9073 - val_loss: 1.0495 - val_accuracy: 0.6861\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2544 - accuracy: 0.9145 - val_loss: 1.0281 - val_accuracy: 0.6836\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2741 - accuracy: 0.9091 - val_loss: 1.1004 - val_accuracy: 0.6727\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2577 - accuracy: 0.9151 - val_loss: 1.0718 - val_accuracy: 0.6812\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2577 - accuracy: 0.9115 - val_loss: 1.0510 - val_accuracy: 0.6885\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2664 - accuracy: 0.9175 - val_loss: 1.0481 - val_accuracy: 0.6715\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2621 - accuracy: 0.9085 - val_loss: 1.0655 - val_accuracy: 0.6752\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2664 - accuracy: 0.9127 - val_loss: 1.0798 - val_accuracy: 0.6788\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2566 - accuracy: 0.9169 - val_loss: 1.0678 - val_accuracy: 0.6788\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2514 - accuracy: 0.9169 - val_loss: 1.0590 - val_accuracy: 0.6861\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2730 - accuracy: 0.9181 - val_loss: 1.0850 - val_accuracy: 0.6788\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2747 - accuracy: 0.9037 - val_loss: 1.0646 - val_accuracy: 0.6812\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2594 - accuracy: 0.9145 - val_loss: 1.1059 - val_accuracy: 0.6727\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2498 - accuracy: 0.9133 - val_loss: 1.0792 - val_accuracy: 0.6739\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2452 - accuracy: 0.9175 - val_loss: 1.0768 - val_accuracy: 0.6836\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2642 - accuracy: 0.9061 - val_loss: 1.0943 - val_accuracy: 0.6703\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2683 - accuracy: 0.9091 - val_loss: 1.1139 - val_accuracy: 0.6788\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2543 - accuracy: 0.9043 - val_loss: 1.0606 - val_accuracy: 0.6788\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2761 - accuracy: 0.9073 - val_loss: 1.0561 - val_accuracy: 0.6776\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2648 - accuracy: 0.9157 - val_loss: 1.0762 - val_accuracy: 0.6861\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2594 - accuracy: 0.9073 - val_loss: 1.0898 - val_accuracy: 0.6691\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2524 - accuracy: 0.9163 - val_loss: 1.0710 - val_accuracy: 0.6776\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2640 - accuracy: 0.9049 - val_loss: 1.0601 - val_accuracy: 0.6776\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2655 - accuracy: 0.9145 - val_loss: 1.0886 - val_accuracy: 0.6836\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2675 - accuracy: 0.9025 - val_loss: 1.0693 - val_accuracy: 0.6800\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2705 - accuracy: 0.9073 - val_loss: 1.0496 - val_accuracy: 0.6861\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2459 - accuracy: 0.9151 - val_loss: 1.0768 - val_accuracy: 0.6679\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2627 - accuracy: 0.9103 - val_loss: 1.1101 - val_accuracy: 0.6776\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2536 - accuracy: 0.9175 - val_loss: 1.0843 - val_accuracy: 0.6630\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2463 - accuracy: 0.9193 - val_loss: 1.0842 - val_accuracy: 0.6921\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2652 - accuracy: 0.9145 - val_loss: 1.0673 - val_accuracy: 0.6752\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2395 - accuracy: 0.9240 - val_loss: 1.1062 - val_accuracy: 0.6909\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2601 - accuracy: 0.9115 - val_loss: 1.0542 - val_accuracy: 0.6861\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2463 - accuracy: 0.9240 - val_loss: 1.1682 - val_accuracy: 0.6606\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2377 - accuracy: 0.9181 - val_loss: 1.0728 - val_accuracy: 0.6812\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2355 - accuracy: 0.9139 - val_loss: 1.0869 - val_accuracy: 0.6873\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2505 - accuracy: 0.9193 - val_loss: 1.0622 - val_accuracy: 0.6933\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2668 - accuracy: 0.9127 - val_loss: 1.0406 - val_accuracy: 0.6800\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2434 - accuracy: 0.9175 - val_loss: 1.0568 - val_accuracy: 0.6909\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2610 - accuracy: 0.9103 - val_loss: 1.0934 - val_accuracy: 0.6836\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2549 - accuracy: 0.9181 - val_loss: 1.0532 - val_accuracy: 0.6824\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2460 - accuracy: 0.9234 - val_loss: 1.0822 - val_accuracy: 0.6873\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2487 - accuracy: 0.9127 - val_loss: 1.1049 - val_accuracy: 0.6909\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2517 - accuracy: 0.9133 - val_loss: 1.0596 - val_accuracy: 0.6824\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2308 - accuracy: 0.9318 - val_loss: 1.1028 - val_accuracy: 0.6752\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2307 - accuracy: 0.9199 - val_loss: 1.0874 - val_accuracy: 0.6836\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2557 - accuracy: 0.9115 - val_loss: 1.0759 - val_accuracy: 0.6848\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2418 - accuracy: 0.9205 - val_loss: 1.1137 - val_accuracy: 0.6715\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2526 - accuracy: 0.9187 - val_loss: 1.1107 - val_accuracy: 0.6739\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2364 - accuracy: 0.9276 - val_loss: 1.0614 - val_accuracy: 0.6970\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2506 - accuracy: 0.9222 - val_loss: 1.0601 - val_accuracy: 0.6885\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2285 - accuracy: 0.9199 - val_loss: 1.0765 - val_accuracy: 0.6994\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2593 - accuracy: 0.9205 - val_loss: 1.0812 - val_accuracy: 0.6921\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2441 - accuracy: 0.9133 - val_loss: 1.1045 - val_accuracy: 0.6788\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2409 - accuracy: 0.9217 - val_loss: 1.0805 - val_accuracy: 0.6836\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2429 - accuracy: 0.9169 - val_loss: 1.0933 - val_accuracy: 0.6885\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2494 - accuracy: 0.9145 - val_loss: 1.0690 - val_accuracy: 0.6958\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2289 - accuracy: 0.9211 - val_loss: 1.0720 - val_accuracy: 0.6715\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2468 - accuracy: 0.9181 - val_loss: 1.0857 - val_accuracy: 0.6885\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2493 - accuracy: 0.9109 - val_loss: 1.0796 - val_accuracy: 0.6848\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2302 - accuracy: 0.9211 - val_loss: 1.0916 - val_accuracy: 0.6800\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2518 - accuracy: 0.9145 - val_loss: 1.0634 - val_accuracy: 0.6824\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2330 - accuracy: 0.9258 - val_loss: 1.0526 - val_accuracy: 0.6788\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2582 - accuracy: 0.9091 - val_loss: 1.0569 - val_accuracy: 0.6836\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2370 - accuracy: 0.9222 - val_loss: 1.1191 - val_accuracy: 0.6691\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2472 - accuracy: 0.9103 - val_loss: 1.0918 - val_accuracy: 0.6812\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2320 - accuracy: 0.9187 - val_loss: 1.1182 - val_accuracy: 0.6848\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2448 - accuracy: 0.9175 - val_loss: 1.0891 - val_accuracy: 0.6739\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.2201 - accuracy: 0.9324 - val_loss: 1.0707 - val_accuracy: 0.6764\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2255 - accuracy: 0.9258 - val_loss: 1.1013 - val_accuracy: 0.6752\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2450 - accuracy: 0.9163 - val_loss: 1.0573 - val_accuracy: 0.6788\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2348 - accuracy: 0.9222 - val_loss: 1.0844 - val_accuracy: 0.6812\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2190 - accuracy: 0.9348 - val_loss: 1.1196 - val_accuracy: 0.6739\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2225 - accuracy: 0.9252 - val_loss: 1.1360 - val_accuracy: 0.6861\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2527 - accuracy: 0.9103 - val_loss: 1.0874 - val_accuracy: 0.6933\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2196 - accuracy: 0.9306 - val_loss: 1.1005 - val_accuracy: 0.6752\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2427 - accuracy: 0.9133 - val_loss: 1.0703 - val_accuracy: 0.6873\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2318 - accuracy: 0.9222 - val_loss: 1.1224 - val_accuracy: 0.6703\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2305 - accuracy: 0.9187 - val_loss: 1.1055 - val_accuracy: 0.6958\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2470 - accuracy: 0.9163 - val_loss: 1.0728 - val_accuracy: 0.6897\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2470 - accuracy: 0.9193 - val_loss: 1.0860 - val_accuracy: 0.6861\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2394 - accuracy: 0.9193 - val_loss: 1.0512 - val_accuracy: 0.6800\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2290 - accuracy: 0.9282 - val_loss: 1.0755 - val_accuracy: 0.6812\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2378 - accuracy: 0.9222 - val_loss: 1.0830 - val_accuracy: 0.6764\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2235 - accuracy: 0.9181 - val_loss: 1.0711 - val_accuracy: 0.6885\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2354 - accuracy: 0.9145 - val_loss: 1.0561 - val_accuracy: 0.6824\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2236 - accuracy: 0.9324 - val_loss: 1.0822 - val_accuracy: 0.6812\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2234 - accuracy: 0.9228 - val_loss: 1.0808 - val_accuracy: 0.6800\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2324 - accuracy: 0.9187 - val_loss: 1.0730 - val_accuracy: 0.6764\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2264 - accuracy: 0.9264 - val_loss: 1.0928 - val_accuracy: 0.6861\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2608 - accuracy: 0.9043 - val_loss: 1.1205 - val_accuracy: 0.6679\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2232 - accuracy: 0.9193 - val_loss: 1.0938 - val_accuracy: 0.6800\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2178 - accuracy: 0.9348 - val_loss: 1.1537 - val_accuracy: 0.6776\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2088 - accuracy: 0.9270 - val_loss: 1.0982 - val_accuracy: 0.6958\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2253 - accuracy: 0.9282 - val_loss: 1.0802 - val_accuracy: 0.6824\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2188 - accuracy: 0.9270 - val_loss: 1.1411 - val_accuracy: 0.6800\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2389 - accuracy: 0.9199 - val_loss: 1.1195 - val_accuracy: 0.6897\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2073 - accuracy: 0.9312 - val_loss: 1.1031 - val_accuracy: 0.6764\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2203 - accuracy: 0.9205 - val_loss: 1.1206 - val_accuracy: 0.6958\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2274 - accuracy: 0.9234 - val_loss: 1.0652 - val_accuracy: 0.6812\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2106 - accuracy: 0.9276 - val_loss: 1.0875 - val_accuracy: 0.6848\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2217 - accuracy: 0.9264 - val_loss: 1.1274 - val_accuracy: 0.6776\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2182 - accuracy: 0.9312 - val_loss: 1.0917 - val_accuracy: 0.6848\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2254 - accuracy: 0.9228 - val_loss: 1.0652 - val_accuracy: 0.6836\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2231 - accuracy: 0.9228 - val_loss: 1.0767 - val_accuracy: 0.6958\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2101 - accuracy: 0.9414 - val_loss: 1.0978 - val_accuracy: 0.6836\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2217 - accuracy: 0.9270 - val_loss: 1.1007 - val_accuracy: 0.6982\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 1s 9ms/step - loss: 0.2088 - accuracy: 0.9276 - val_loss: 1.1029 - val_accuracy: 0.6958\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2418 - accuracy: 0.9246 - val_loss: 1.0734 - val_accuracy: 0.6909\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2444 - accuracy: 0.9145 - val_loss: 1.1124 - val_accuracy: 0.6885\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2265 - accuracy: 0.9246 - val_loss: 1.0749 - val_accuracy: 0.6836\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2376 - accuracy: 0.9151 - val_loss: 1.0999 - val_accuracy: 0.6897\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2166 - accuracy: 0.9288 - val_loss: 1.0891 - val_accuracy: 0.6800\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2210 - accuracy: 0.9246 - val_loss: 1.0741 - val_accuracy: 0.6824\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2271 - accuracy: 0.9276 - val_loss: 1.0857 - val_accuracy: 0.6848\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2148 - accuracy: 0.9252 - val_loss: 1.1133 - val_accuracy: 0.6909\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1866 - accuracy: 0.9408 - val_loss: 1.0963 - val_accuracy: 0.6776\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2196 - accuracy: 0.9175 - val_loss: 1.1082 - val_accuracy: 0.6848\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2316 - accuracy: 0.9234 - val_loss: 1.0893 - val_accuracy: 0.6873\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2153 - accuracy: 0.9288 - val_loss: 1.1042 - val_accuracy: 0.6970\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2140 - accuracy: 0.9294 - val_loss: 1.1014 - val_accuracy: 0.6909\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2182 - accuracy: 0.9252 - val_loss: 1.0882 - val_accuracy: 0.6909\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2105 - accuracy: 0.9252 - val_loss: 1.1603 - val_accuracy: 0.6812\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2221 - accuracy: 0.9211 - val_loss: 1.1077 - val_accuracy: 0.6848\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2061 - accuracy: 0.9330 - val_loss: 1.1277 - val_accuracy: 0.6715\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2363 - accuracy: 0.9187 - val_loss: 1.0920 - val_accuracy: 0.6945\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2025 - accuracy: 0.9306 - val_loss: 1.1171 - val_accuracy: 0.6861\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2147 - accuracy: 0.9240 - val_loss: 1.1065 - val_accuracy: 0.6885\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2073 - accuracy: 0.9402 - val_loss: 1.1211 - val_accuracy: 0.6861\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2162 - accuracy: 0.9211 - val_loss: 1.0744 - val_accuracy: 0.6933\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2121 - accuracy: 0.9234 - val_loss: 1.0915 - val_accuracy: 0.6812\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2307 - accuracy: 0.9270 - val_loss: 1.0998 - val_accuracy: 0.6945\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2208 - accuracy: 0.9252 - val_loss: 1.0943 - val_accuracy: 0.6982\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2167 - accuracy: 0.9252 - val_loss: 1.1159 - val_accuracy: 0.6897\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2110 - accuracy: 0.9324 - val_loss: 1.0871 - val_accuracy: 0.6982\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2027 - accuracy: 0.9318 - val_loss: 1.1165 - val_accuracy: 0.6824\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2125 - accuracy: 0.9252 - val_loss: 1.1245 - val_accuracy: 0.6873\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2082 - accuracy: 0.9336 - val_loss: 1.1452 - val_accuracy: 0.6945\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2174 - accuracy: 0.9300 - val_loss: 1.0851 - val_accuracy: 0.6970\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1941 - accuracy: 0.9366 - val_loss: 1.1263 - val_accuracy: 0.6909\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2170 - accuracy: 0.9300 - val_loss: 1.1540 - val_accuracy: 0.6776\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2008 - accuracy: 0.9324 - val_loss: 1.0942 - val_accuracy: 0.6958\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2065 - accuracy: 0.9366 - val_loss: 1.1609 - val_accuracy: 0.6921\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2229 - accuracy: 0.9228 - val_loss: 1.0898 - val_accuracy: 0.6848\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2085 - accuracy: 0.9336 - val_loss: 1.1186 - val_accuracy: 0.6885\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2196 - accuracy: 0.9300 - val_loss: 1.0873 - val_accuracy: 0.6970\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1886 - accuracy: 0.9390 - val_loss: 1.1109 - val_accuracy: 0.6861\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2097 - accuracy: 0.9264 - val_loss: 1.1414 - val_accuracy: 0.6897\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2163 - accuracy: 0.9252 - val_loss: 1.0864 - val_accuracy: 0.6836\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2111 - accuracy: 0.9282 - val_loss: 1.1132 - val_accuracy: 0.6897\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2003 - accuracy: 0.9378 - val_loss: 1.1513 - val_accuracy: 0.6800\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1875 - accuracy: 0.9390 - val_loss: 1.1254 - val_accuracy: 0.6836\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1984 - accuracy: 0.9366 - val_loss: 1.1408 - val_accuracy: 0.6788\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2170 - accuracy: 0.9270 - val_loss: 1.1541 - val_accuracy: 0.6873\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2092 - accuracy: 0.9348 - val_loss: 1.1239 - val_accuracy: 0.7006\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1995 - accuracy: 0.9324 - val_loss: 1.1385 - val_accuracy: 0.6885\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1990 - accuracy: 0.9390 - val_loss: 1.0837 - val_accuracy: 0.7018\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2102 - accuracy: 0.9222 - val_loss: 1.1283 - val_accuracy: 0.6873\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1968 - accuracy: 0.9360 - val_loss: 1.1257 - val_accuracy: 0.6921\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2096 - accuracy: 0.9318 - val_loss: 1.1594 - val_accuracy: 0.7030\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2127 - accuracy: 0.9258 - val_loss: 1.1405 - val_accuracy: 0.6788\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1903 - accuracy: 0.9468 - val_loss: 1.1469 - val_accuracy: 0.7018\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2071 - accuracy: 0.9282 - val_loss: 1.1251 - val_accuracy: 0.6873\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1999 - accuracy: 0.9318 - val_loss: 1.1501 - val_accuracy: 0.6715\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1977 - accuracy: 0.9294 - val_loss: 1.1312 - val_accuracy: 0.6909\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1916 - accuracy: 0.9354 - val_loss: 1.1466 - val_accuracy: 0.6921\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2132 - accuracy: 0.9264 - val_loss: 1.1201 - val_accuracy: 0.6764\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1920 - accuracy: 0.9294 - val_loss: 1.1275 - val_accuracy: 0.6848\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2095 - accuracy: 0.9330 - val_loss: 1.1599 - val_accuracy: 0.6970\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1890 - accuracy: 0.9336 - val_loss: 1.1169 - val_accuracy: 0.6933\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1987 - accuracy: 0.9372 - val_loss: 1.1244 - val_accuracy: 0.6848\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2005 - accuracy: 0.9324 - val_loss: 1.1172 - val_accuracy: 0.6824\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1887 - accuracy: 0.9438 - val_loss: 1.1402 - val_accuracy: 0.6861\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2000 - accuracy: 0.9318 - val_loss: 1.1116 - val_accuracy: 0.6945\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1930 - accuracy: 0.9342 - val_loss: 1.1228 - val_accuracy: 0.6752\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1888 - accuracy: 0.9390 - val_loss: 1.1394 - val_accuracy: 0.6873\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2059 - accuracy: 0.9330 - val_loss: 1.1399 - val_accuracy: 0.7006\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.2006 - accuracy: 0.9336 - val_loss: 1.1328 - val_accuracy: 0.6800\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1876 - accuracy: 0.9384 - val_loss: 1.1098 - val_accuracy: 0.6933\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.2087 - accuracy: 0.9294 - val_loss: 1.1170 - val_accuracy: 0.6788\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1889 - accuracy: 0.9414 - val_loss: 1.1095 - val_accuracy: 0.6897\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1857 - accuracy: 0.9348 - val_loss: 1.1359 - val_accuracy: 0.6909\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1904 - accuracy: 0.9306 - val_loss: 1.1590 - val_accuracy: 0.6812\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1987 - accuracy: 0.9324 - val_loss: 1.1466 - val_accuracy: 0.6982\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1949 - accuracy: 0.9282 - val_loss: 1.1396 - val_accuracy: 0.6909\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1926 - accuracy: 0.9348 - val_loss: 1.1680 - val_accuracy: 0.6812\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 1s 10ms/step - loss: 0.1945 - accuracy: 0.9354 - val_loss: 1.1349 - val_accuracy: 0.6824\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 1s 11ms/step - loss: 0.1921 - accuracy: 0.9432 - val_loss: 1.1446 - val_accuracy: 0.6873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nan2EEDdb0LG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f9e27c3b-25fc-4404-9609-9afb1fa5e0ab"
      },
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFG62rnkcji1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "18a9c437-bac7-49eb-a0a1-458f8c393db4"
      },
      "source": [
        "plt.plot(cnnhistory.history['accuracy'])\n",
        "plt.plot(cnnhistory.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObBSAORgdqt2"
      },
      "source": [
        "predictions = np.argmax(model.predict(x_testcnn),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRzVSvCKdt6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1550df5-fcb7-4a90-8458-32ed75291a8c"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3, 3, 0, 2, 4, 3, 7, 1, 1, 5, 2, 4, 4, 0, 1, 6, 5, 6, 4, 3, 2,\n",
              "       6, 4, 5, 7, 2, 5, 1, 5, 2, 5, 1, 5, 3, 2, 7, 5, 1, 2, 1, 3, 7, 4,\n",
              "       4, 1, 3, 0, 1, 1, 1, 0, 3, 4, 0, 0, 2, 4, 1, 0, 2, 7, 4, 0, 5, 6,\n",
              "       7, 0, 7, 3, 4, 4, 6, 3, 4, 7, 3, 3, 3, 6, 7, 1, 2, 3, 7, 5, 5, 5,\n",
              "       5, 5, 4, 1, 3, 1, 1, 2, 5, 5, 6, 2, 2, 6, 4, 3, 0, 2, 2, 3, 5, 4,\n",
              "       3, 1, 4, 0, 4, 3, 1, 3, 5, 4, 3, 2, 5, 5, 6, 0, 5, 7, 4, 5, 5, 3,\n",
              "       4, 4, 3, 5, 1, 7, 4, 5, 5, 6, 7, 5, 3, 1, 1, 5, 4, 5, 5, 6, 4, 5,\n",
              "       6, 5, 3, 2, 5, 6, 7, 1, 3, 1, 3, 4, 1, 7, 4, 1, 6, 7, 7, 3, 1, 0,\n",
              "       0, 5, 5, 1, 3, 7, 7, 3, 1, 5, 7, 6, 1, 3, 2, 5, 5, 6, 4, 2, 4, 1,\n",
              "       3, 1, 4, 7, 5, 6, 0, 7, 5, 7, 4, 5, 4, 1, 3, 7, 7, 5, 4, 1, 0, 2,\n",
              "       6, 4, 1, 1, 1, 2, 4, 3, 1, 1, 7, 7, 6, 2, 5, 4, 2, 6, 5, 2, 1, 4,\n",
              "       1, 1, 1, 5, 3, 5, 5, 5, 1, 6, 2, 1, 4, 2, 3, 4, 4, 5, 3, 1, 3, 5,\n",
              "       1, 3, 0, 2, 2, 2, 3, 5, 0, 7, 2, 1, 1, 1, 2, 2, 1, 5, 4, 5, 5, 5,\n",
              "       1, 2, 5, 5, 2, 1, 0, 4, 1, 2, 2, 4, 5, 4, 2, 3, 4, 6, 4, 5, 6, 0,\n",
              "       1, 3, 1, 4, 1, 4, 4, 2, 2, 6, 5, 0, 2, 3, 1, 4, 1, 1, 2, 1, 4, 6,\n",
              "       6, 0, 5, 7, 1, 5, 5, 2, 4, 3, 6, 4, 4, 5, 6, 6, 2, 0, 2, 4, 1, 5,\n",
              "       0, 5, 2, 1, 4, 4, 7, 3, 6, 2, 5, 2, 4, 3, 4, 1, 1, 1, 4, 6, 4, 1,\n",
              "       1, 0, 5, 2, 7, 2, 7, 0, 5, 1, 3, 7, 2, 3, 5, 7, 3, 4, 3, 0, 2, 0,\n",
              "       5, 2, 3, 3, 2, 5, 2, 4, 4, 3, 6, 4, 5, 7, 5, 3, 1, 1, 4, 5, 7, 1,\n",
              "       0, 1, 5, 2, 2, 1, 5, 2, 1, 1, 4, 4, 3, 1, 1, 5, 0, 3, 5, 7, 3, 1,\n",
              "       2, 3, 0, 7, 1, 5, 2, 6, 5, 5, 2, 2, 4, 6, 7, 5, 1, 7, 2, 4, 1, 5,\n",
              "       5, 1, 7, 5, 5, 5, 5, 4, 0, 2, 5, 7, 3, 3, 4, 5, 5, 5, 4, 3, 3, 1,\n",
              "       1, 4, 2, 1, 7, 2, 4, 1, 1, 5, 2, 5, 2, 3, 5, 5, 3, 5, 5, 2, 3, 2,\n",
              "       6, 5, 2, 5, 7, 2, 5, 1, 5, 0, 1, 6, 5, 2, 4, 6, 1, 4, 0, 0, 3, 0,\n",
              "       7, 7, 5, 0, 5, 5, 1, 3, 7, 3, 0, 4, 5, 3, 7, 1, 1, 2, 0, 4, 0, 1,\n",
              "       5, 2, 2, 5, 1, 4, 4, 1, 1, 5, 5, 3, 2, 3, 5, 4, 4, 6, 7, 7, 4, 4,\n",
              "       1, 5, 7, 4, 1, 5, 3, 5, 1, 5, 5, 5, 3, 1, 0, 5, 3, 7, 3, 3, 5, 5,\n",
              "       2, 2, 2, 2, 5, 1, 3, 1, 5, 2, 2, 3, 7, 5, 5, 0, 2, 4, 4, 2, 6, 1,\n",
              "       3, 3, 3, 2, 5, 2, 2, 1, 6, 2, 2, 3, 4, 1, 5, 1, 7, 4, 6, 2, 7, 0,\n",
              "       2, 3, 4, 1, 1, 2, 2, 5, 0, 5, 0, 3, 5, 2, 5, 3, 7, 6, 1, 4, 0, 2,\n",
              "       7, 4, 1, 5, 5, 4, 3, 3, 2, 5, 3, 6, 6, 5, 2, 5, 1, 6, 5, 5, 1, 4,\n",
              "       5, 5, 2, 1, 4, 5, 4, 1, 5, 5, 7, 6, 7, 5, 0, 3, 7, 3, 2, 0, 6, 4,\n",
              "       2, 5, 2, 4, 0, 5, 1, 5, 4, 1, 6, 6, 2, 1, 6, 4, 2, 4, 2, 2, 0, 3,\n",
              "       5, 3, 6, 2, 7, 1, 5, 0, 7, 6, 3, 5, 5, 2, 6, 7, 5, 1, 1, 2, 4, 1,\n",
              "       2, 2, 5, 2, 0, 7, 5, 1, 1, 3, 2, 3, 2, 5, 1, 6, 5, 4, 2, 6, 1, 7,\n",
              "       3, 1, 4, 1, 5, 5, 1, 3, 1, 4, 1, 5, 6, 2, 5, 3, 7, 5, 0, 2, 3, 5,\n",
              "       0, 5, 4, 3, 1, 0, 5, 4, 0, 5, 4, 1, 1, 7, 1, 6, 3, 2, 2, 3, 1, 2,\n",
              "       2, 1, 1, 4, 3, 3, 7, 6, 1, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JQldD_3dxCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1458d3ce-fb20-4429-f2a7-f09ceecd730f"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3, 3, 0, 6, 7, 3, 7, 0, 4, 4, 2, 4, 4, 0, 1, 6, 5, 6, 4, 0, 2,\n",
              "       6, 4, 5, 7, 2, 5, 3, 6, 2, 5, 3, 5, 3, 2, 7, 3, 1, 2, 1, 3, 3, 2,\n",
              "       2, 3, 3, 0, 1, 1, 1, 0, 3, 4, 0, 0, 2, 4, 3, 0, 2, 7, 4, 0, 5, 6,\n",
              "       7, 0, 7, 3, 5, 2, 6, 1, 4, 7, 5, 0, 3, 0, 7, 1, 2, 3, 7, 5, 6, 5,\n",
              "       3, 5, 4, 1, 3, 1, 1, 2, 5, 6, 6, 4, 2, 6, 4, 3, 0, 6, 2, 5, 5, 4,\n",
              "       3, 1, 4, 1, 4, 3, 3, 0, 0, 4, 3, 2, 5, 5, 6, 0, 5, 7, 4, 5, 5, 3,\n",
              "       6, 4, 1, 5, 1, 7, 4, 2, 6, 3, 7, 5, 3, 1, 1, 5, 4, 5, 5, 5, 4, 1,\n",
              "       7, 4, 3, 2, 5, 6, 7, 1, 7, 3, 1, 4, 1, 6, 4, 1, 6, 7, 2, 3, 1, 0,\n",
              "       0, 3, 5, 1, 5, 7, 7, 3, 2, 2, 2, 6, 1, 3, 6, 5, 1, 6, 4, 2, 4, 3,\n",
              "       3, 0, 2, 5, 3, 7, 0, 5, 5, 7, 4, 5, 4, 1, 5, 7, 7, 3, 4, 1, 0, 5,\n",
              "       6, 4, 0, 1, 1, 2, 4, 0, 4, 1, 7, 7, 3, 7, 5, 4, 4, 6, 5, 7, 2, 4,\n",
              "       1, 1, 1, 2, 3, 5, 2, 3, 1, 2, 2, 3, 6, 2, 3, 7, 4, 5, 3, 6, 4, 5,\n",
              "       1, 3, 0, 2, 2, 2, 3, 5, 0, 7, 2, 1, 1, 3, 2, 2, 0, 5, 4, 3, 0, 7,\n",
              "       4, 6, 5, 7, 2, 1, 0, 4, 1, 2, 2, 4, 5, 4, 2, 3, 4, 6, 4, 5, 2, 7,\n",
              "       1, 3, 1, 4, 1, 4, 4, 2, 2, 4, 5, 0, 7, 0, 1, 4, 1, 1, 7, 1, 4, 6,\n",
              "       5, 0, 5, 2, 1, 4, 5, 2, 4, 3, 7, 4, 4, 5, 3, 6, 2, 0, 2, 2, 1, 5,\n",
              "       1, 3, 2, 1, 4, 4, 5, 3, 4, 4, 2, 2, 4, 5, 4, 1, 1, 1, 4, 6, 4, 1,\n",
              "       3, 0, 5, 2, 7, 2, 7, 0, 5, 1, 3, 7, 1, 3, 5, 7, 7, 4, 1, 7, 1, 5,\n",
              "       3, 2, 5, 3, 2, 0, 1, 6, 6, 3, 7, 4, 7, 3, 5, 5, 1, 1, 2, 5, 3, 6,\n",
              "       0, 1, 5, 2, 2, 0, 5, 7, 1, 1, 4, 4, 3, 2, 1, 5, 0, 3, 3, 6, 3, 2,\n",
              "       2, 6, 0, 7, 1, 6, 2, 6, 5, 5, 6, 2, 4, 6, 7, 5, 1, 5, 2, 4, 3, 6,\n",
              "       5, 1, 5, 5, 5, 2, 5, 4, 0, 2, 4, 1, 3, 3, 4, 7, 3, 5, 4, 5, 3, 1,\n",
              "       1, 4, 2, 1, 2, 7, 4, 3, 1, 2, 5, 5, 2, 3, 5, 3, 6, 5, 6, 2, 5, 2,\n",
              "       6, 5, 2, 5, 5, 6, 5, 1, 5, 0, 1, 2, 4, 2, 4, 7, 1, 4, 3, 0, 3, 0,\n",
              "       7, 7, 6, 0, 5, 5, 0, 0, 7, 3, 7, 4, 5, 3, 7, 1, 1, 2, 0, 4, 0, 2,\n",
              "       5, 2, 7, 2, 1, 4, 2, 1, 1, 5, 3, 3, 5, 0, 3, 4, 4, 6, 6, 7, 4, 4,\n",
              "       1, 5, 7, 4, 1, 5, 5, 5, 1, 5, 7, 3, 5, 1, 7, 5, 3, 7, 5, 3, 3, 5,\n",
              "       1, 5, 2, 2, 7, 6, 7, 7, 5, 2, 2, 3, 7, 5, 2, 0, 2, 4, 4, 4, 6, 3,\n",
              "       3, 2, 3, 2, 5, 5, 2, 1, 6, 2, 2, 3, 4, 1, 3, 1, 7, 5, 6, 5, 2, 0,\n",
              "       2, 3, 4, 1, 3, 2, 2, 5, 0, 7, 5, 5, 4, 3, 3, 3, 6, 7, 1, 4, 0, 2,\n",
              "       7, 4, 1, 5, 5, 4, 3, 7, 2, 5, 3, 6, 6, 4, 2, 5, 1, 6, 4, 3, 3, 4,\n",
              "       5, 3, 2, 1, 4, 2, 5, 1, 5, 5, 7, 0, 7, 5, 0, 0, 0, 3, 2, 3, 3, 4,\n",
              "       2, 5, 4, 4, 2, 5, 1, 6, 6, 1, 4, 6, 1, 1, 6, 4, 4, 2, 2, 2, 0, 5,\n",
              "       5, 3, 6, 2, 7, 1, 5, 7, 7, 6, 0, 2, 5, 2, 6, 0, 5, 1, 7, 2, 4, 1,\n",
              "       2, 2, 3, 6, 0, 7, 5, 1, 1, 3, 6, 3, 2, 3, 1, 4, 2, 4, 6, 3, 1, 7,\n",
              "       3, 1, 4, 1, 5, 5, 1, 3, 1, 4, 1, 3, 7, 7, 5, 3, 4, 0, 0, 2, 2, 5,\n",
              "       0, 5, 4, 3, 0, 0, 7, 4, 0, 5, 4, 1, 3, 7, 3, 6, 7, 2, 5, 3, 0, 4,\n",
              "       2, 1, 1, 4, 4, 3, 0, 6, 2, 3, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYO3O1I0dxJw"
      },
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-KAOWdhdxNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9203a9ea-e539-4572-e634-3bb1e12558f0"
      },
      "source": [
        "new_Ytest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3, 3, 0, 6, 7, 3, 7, 0, 4, 4, 2, 4, 4, 0, 1, 6, 5, 6, 4, 0, 2,\n",
              "       6, 4, 5, 7, 2, 5, 3, 6, 2, 5, 3, 5, 3, 2, 7, 3, 1, 2, 1, 3, 3, 2,\n",
              "       2, 3, 3, 0, 1, 1, 1, 0, 3, 4, 0, 0, 2, 4, 3, 0, 2, 7, 4, 0, 5, 6,\n",
              "       7, 0, 7, 3, 5, 2, 6, 1, 4, 7, 5, 0, 3, 0, 7, 1, 2, 3, 7, 5, 6, 5,\n",
              "       3, 5, 4, 1, 3, 1, 1, 2, 5, 6, 6, 4, 2, 6, 4, 3, 0, 6, 2, 5, 5, 4,\n",
              "       3, 1, 4, 1, 4, 3, 3, 0, 0, 4, 3, 2, 5, 5, 6, 0, 5, 7, 4, 5, 5, 3,\n",
              "       6, 4, 1, 5, 1, 7, 4, 2, 6, 3, 7, 5, 3, 1, 1, 5, 4, 5, 5, 5, 4, 1,\n",
              "       7, 4, 3, 2, 5, 6, 7, 1, 7, 3, 1, 4, 1, 6, 4, 1, 6, 7, 2, 3, 1, 0,\n",
              "       0, 3, 5, 1, 5, 7, 7, 3, 2, 2, 2, 6, 1, 3, 6, 5, 1, 6, 4, 2, 4, 3,\n",
              "       3, 0, 2, 5, 3, 7, 0, 5, 5, 7, 4, 5, 4, 1, 5, 7, 7, 3, 4, 1, 0, 5,\n",
              "       6, 4, 0, 1, 1, 2, 4, 0, 4, 1, 7, 7, 3, 7, 5, 4, 4, 6, 5, 7, 2, 4,\n",
              "       1, 1, 1, 2, 3, 5, 2, 3, 1, 2, 2, 3, 6, 2, 3, 7, 4, 5, 3, 6, 4, 5,\n",
              "       1, 3, 0, 2, 2, 2, 3, 5, 0, 7, 2, 1, 1, 3, 2, 2, 0, 5, 4, 3, 0, 7,\n",
              "       4, 6, 5, 7, 2, 1, 0, 4, 1, 2, 2, 4, 5, 4, 2, 3, 4, 6, 4, 5, 2, 7,\n",
              "       1, 3, 1, 4, 1, 4, 4, 2, 2, 4, 5, 0, 7, 0, 1, 4, 1, 1, 7, 1, 4, 6,\n",
              "       5, 0, 5, 2, 1, 4, 5, 2, 4, 3, 7, 4, 4, 5, 3, 6, 2, 0, 2, 2, 1, 5,\n",
              "       1, 3, 2, 1, 4, 4, 5, 3, 4, 4, 2, 2, 4, 5, 4, 1, 1, 1, 4, 6, 4, 1,\n",
              "       3, 0, 5, 2, 7, 2, 7, 0, 5, 1, 3, 7, 1, 3, 5, 7, 7, 4, 1, 7, 1, 5,\n",
              "       3, 2, 5, 3, 2, 0, 1, 6, 6, 3, 7, 4, 7, 3, 5, 5, 1, 1, 2, 5, 3, 6,\n",
              "       0, 1, 5, 2, 2, 0, 5, 7, 1, 1, 4, 4, 3, 2, 1, 5, 0, 3, 3, 6, 3, 2,\n",
              "       2, 6, 0, 7, 1, 6, 2, 6, 5, 5, 6, 2, 4, 6, 7, 5, 1, 5, 2, 4, 3, 6,\n",
              "       5, 1, 5, 5, 5, 2, 5, 4, 0, 2, 4, 1, 3, 3, 4, 7, 3, 5, 4, 5, 3, 1,\n",
              "       1, 4, 2, 1, 2, 7, 4, 3, 1, 2, 5, 5, 2, 3, 5, 3, 6, 5, 6, 2, 5, 2,\n",
              "       6, 5, 2, 5, 5, 6, 5, 1, 5, 0, 1, 2, 4, 2, 4, 7, 1, 4, 3, 0, 3, 0,\n",
              "       7, 7, 6, 0, 5, 5, 0, 0, 7, 3, 7, 4, 5, 3, 7, 1, 1, 2, 0, 4, 0, 2,\n",
              "       5, 2, 7, 2, 1, 4, 2, 1, 1, 5, 3, 3, 5, 0, 3, 4, 4, 6, 6, 7, 4, 4,\n",
              "       1, 5, 7, 4, 1, 5, 5, 5, 1, 5, 7, 3, 5, 1, 7, 5, 3, 7, 5, 3, 3, 5,\n",
              "       1, 5, 2, 2, 7, 6, 7, 7, 5, 2, 2, 3, 7, 5, 2, 0, 2, 4, 4, 4, 6, 3,\n",
              "       3, 2, 3, 2, 5, 5, 2, 1, 6, 2, 2, 3, 4, 1, 3, 1, 7, 5, 6, 5, 2, 0,\n",
              "       2, 3, 4, 1, 3, 2, 2, 5, 0, 7, 5, 5, 4, 3, 3, 3, 6, 7, 1, 4, 0, 2,\n",
              "       7, 4, 1, 5, 5, 4, 3, 7, 2, 5, 3, 6, 6, 4, 2, 5, 1, 6, 4, 3, 3, 4,\n",
              "       5, 3, 2, 1, 4, 2, 5, 1, 5, 5, 7, 0, 7, 5, 0, 0, 0, 3, 2, 3, 3, 4,\n",
              "       2, 5, 4, 4, 2, 5, 1, 6, 6, 1, 4, 6, 1, 1, 6, 4, 4, 2, 2, 2, 0, 5,\n",
              "       5, 3, 6, 2, 7, 1, 5, 7, 7, 6, 0, 2, 5, 2, 6, 0, 5, 1, 7, 2, 4, 1,\n",
              "       2, 2, 3, 6, 0, 7, 5, 1, 1, 3, 6, 3, 2, 3, 1, 4, 2, 4, 6, 3, 1, 7,\n",
              "       3, 1, 4, 1, 5, 5, 1, 3, 1, 4, 1, 3, 7, 7, 5, 3, 4, 0, 0, 2, 2, 5,\n",
              "       0, 5, 4, 3, 0, 0, 7, 4, 0, 5, 4, 1, 3, 7, 3, 6, 7, 2, 5, 3, 0, 4,\n",
              "       2, 1, 1, 4, 4, 3, 0, 6, 2, 3, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vam_nFy4d73s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351606d6-5146-451a-9651-4ea9c66fe218"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(new_Ytest, predictions)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.63      0.70        70\n",
            "           1       0.72      0.88      0.79       115\n",
            "           2       0.69      0.69      0.69       121\n",
            "           3       0.63      0.56      0.59       118\n",
            "           4       0.84      0.79      0.81       117\n",
            "           5       0.60      0.75      0.66       134\n",
            "           6       0.60      0.52      0.56        67\n",
            "           7       0.66      0.54      0.60        83\n",
            "\n",
            "    accuracy                           0.69       825\n",
            "   macro avg       0.69      0.67      0.68       825\n",
            "weighted avg       0.69      0.69      0.68       825\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVeZHjI1eCR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6ed973-488f-48cd-ee27-0752c4ed22c6"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(new_Ytest, predictions)\n",
        "print (matrix)\n",
        "\n",
        "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 44   8   0   9   0   4   2   3]\n",
            " [  2 101   5   4   0   2   0   1]\n",
            " [  1   6  84   2   8  12   3   5]\n",
            " [  2  17   1  66   0  24   5   3]\n",
            " [  0   3   7   2  92   8   4   1]\n",
            " [  2   0   7  14   3 100   2   6]\n",
            " [  0   3   9   2   5   9  35   4]\n",
            " [  5   2   8   5   2   9   7  45]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Fe0l1gnvRL"
      },
      "source": [
        "**Enregistrer le model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tAm_SlJny_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70b7955-c004-4782-a478-6e1920352767"
      },
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juyZxHm0oB0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb0ba77-701f-4ffa-9cbf-fca23a702f21"
      },
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLCcI9IDoHlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ed0cdd-fb70-4994-c0f3-c1065a40b6a2"
      },
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1446 - accuracy: 0.6873\n",
            "Restored model, accuracy: 68.73%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}